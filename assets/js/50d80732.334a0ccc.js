"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[4492],{4932:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>h,frontMatter:()=>t,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"module3/chapter3-vslam","title":"Chapter 3: Isaac ROS VSLAM","description":"Overview","source":"@site/docs/module3/chapter3-vslam.md","sourceDirName":"module3","slug":"/module3/chapter3-vslam","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/docs/module3/chapter3-vslam","draft":false,"unlisted":false,"editUrl":"https://github.com/shabi9999/Physical-AI-Humanoid-Robotics-Textbook/tree/main/my-website/docs/module3/chapter3-vslam.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"docs","previous":{"title":"Chapter 2: Synthetic Data Generation","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/docs/module3/chapter2-synthetic-data"},"next":{"title":"Chapter 4: Nav2 Path Planning","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/docs/module3/chapter4-nav2"}}');var o=r(4848),i=r(8453);const t={sidebar_position:3},l="Chapter 3: Isaac ROS VSLAM",a={},c=[{value:"Overview",id:"overview",level:2},{value:"What You&#39;ll Learn",id:"what-youll-learn",level:2},{value:"The Problem VSLAM Solves",id:"the-problem-vslam-solves",level:2},{value:"How VSLAM Works: Three Key Concepts",id:"how-vslam-works-three-key-concepts",level:2},{value:"Concept 1: Visual Odometry",id:"concept-1-visual-odometry",level:3},{value:"The Process",id:"the-process",level:4},{value:"Why It Works",id:"why-it-works",level:4},{value:"Errors Accumulate Over Time",id:"errors-accumulate-over-time",level:4},{value:"Concept 2: Loop Closure Detection",id:"concept-2-loop-closure-detection",level:3},{value:"The Process",id:"the-process-1",level:4},{value:"How Recognition Works",id:"how-recognition-works",level:4},{value:"Concept 3: Map Building",id:"concept-3-map-building",level:3},{value:"Types of Maps",id:"types-of-maps",level:4},{value:"The Map Creation Process",id:"the-map-creation-process",level:4},{value:"Isaac ROS VSLAM Pipeline",id:"isaac-ros-vslam-pipeline",level:2},{value:"Pipeline Components",id:"pipeline-components",level:3},{value:"ROS 2 Integration",id:"ros-2-integration",level:3},{value:"Real-World Example: Robot Mapping a Building",id:"real-world-example-robot-mapping-a-building",level:2},{value:"Limitations of VSLAM",id:"limitations-of-vslam",level:2},{value:"Challenge 1: Featureless Environments",id:"challenge-1-featureless-environments",level:3},{value:"Challenge 2: Rapid Lighting Changes",id:"challenge-2-rapid-lighting-changes",level:3},{value:"Challenge 3: Dynamic Objects (Moving People)",id:"challenge-3-dynamic-objects-moving-people",level:3},{value:"Challenge 4: Computationally Intensive",id:"challenge-4-computationally-intensive",level:3},{value:"Comparison: VSLAM vs. Other Localization Methods",id:"comparison-vslam-vs-other-localization-methods",level:2},{value:"What Happens Next",id:"what-happens-next",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Next: Chapter 4",id:"next-chapter-4",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"chapter-3-isaac-ros-vslam",children:"Chapter 3: Isaac ROS VSLAM"})}),"\n",(0,o.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,o.jsxs)(n.p,{children:["In this chapter, you'll learn how humanoid robots use ",(0,o.jsx)(n.strong,{children:"Visual Simultaneous Localization and Mapping (VSLAM)"})," to figure out where they are and build a map of their environment using only camera input\u2014no GPS, no pre-made maps, just images."]}),"\n",(0,o.jsx)(n.h2,{id:"what-youll-learn",children:"What You'll Learn"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"What VSLAM is and why it's revolutionary for indoor robotics"}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Visual Odometry"}),": How robots track their movement from image to image"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Loop Closure Detection"}),": How robots recognize familiar places to correct drift"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Map Building"}),": How robots create 3D representations of their environment"]}),"\n",(0,o.jsx)(n.li,{children:"The Isaac ROS VSLAM pipeline: from camera input to pose estimation"}),"\n",(0,o.jsx)(n.li,{children:"Real-world scenarios where VSLAM enables robot autonomy"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"the-problem-vslam-solves",children:"The Problem VSLAM Solves"}),"\n",(0,o.jsx)(n.p,{children:"Imagine a humanoid robot exploring an unknown warehouse:"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Without VSLAM"}),":"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Robot starts at position (0, 0)"}),"\n",(0,o.jsx)(n.li,{children:"Moves forward 10 meters"}),"\n",(0,o.jsx)(n.li,{children:'Motor encoders say "10 meters traveled"\u2014but they accumulate error'}),"\n",(0,o.jsx)(n.li,{children:"Robot thinks it's at position (10, 0)\u2014but in reality it's at (9.8, 0.1)"}),"\n",(0,o.jsx)(n.li,{children:"After 10 movements, position error: 1-2 meters"}),"\n",(0,o.jsx)(n.li,{children:"Robot gets lost!"}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"With VSLAM"}),":"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Camera observes distinctive features (corners, edges, patterns)"}),"\n",(0,o.jsx)(n.li,{children:"As robot moves, camera tracks those features across frames"}),"\n",(0,o.jsx)(n.li,{children:"When robot revisits an area (loop closure), camera recognizes it!"}),"\n",(0,o.jsx)(n.li,{children:'Map is self-correcting: robot realizes "I\'ve been here before"'}),"\n",(0,o.jsx)(n.li,{children:"Position error stays under 10 centimeters even after 100 meters of movement"}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:'VSLAM transforms a robot from "lost after 10 meters" to "confident after 100+ meters."'}),"\n",(0,o.jsx)(n.h2,{id:"how-vslam-works-three-key-concepts",children:"How VSLAM Works: Three Key Concepts"}),"\n",(0,o.jsx)(n.h3,{id:"concept-1-visual-odometry",children:"Concept 1: Visual Odometry"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Visual odometry"})," estimates how far a robot has moved by analyzing changes between image frames."]}),"\n",(0,o.jsx)(n.h4,{id:"the-process",children:"The Process"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"Frame 1 (robot at position A):\n[image] \u2192 Detect features: corner at (100, 150), edge at (200, 300)\n\nFrame 2 (robot moved forward):\n[image] \u2192 Same corner now at (80, 150), edge now at (190, 310)\n\nAnalysis:\nCorner moved left \u2192 camera moved right (robot moved forward)\nChange in position = atan2(pixel shift) \u2192 distance traveled\n"})}),"\n",(0,o.jsx)(n.h4,{id:"why-it-works",children:"Why It Works"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Features are consistent"}),": The same physical corner appears in sequential frames"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Movement is predictable"}),": Small motion between consecutive frames"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Math is robust"}),": Even with some feature mismatches, overall motion is accurate"]}),"\n"]}),"\n",(0,o.jsx)(n.h4,{id:"errors-accumulate-over-time",children:"Errors Accumulate Over Time"}),"\n",(0,o.jsx)(n.p,{children:"Visual odometry's weakness:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Each frame introduces small errors (misdetected features, lighting changes)"}),"\n",(0,o.jsx)(n.li,{children:"Errors accumulate: 1st frame \xb11cm, 2nd frame \xb11cm \u2192 total \xb12cm"}),"\n",(0,o.jsx)(n.li,{children:"After 100 frames: potential error of 1 meter!"}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Solution"}),": Loop closure detection (next concept)."]}),"\n",(0,o.jsx)(n.h3,{id:"concept-2-loop-closure-detection",children:"Concept 2: Loop Closure Detection"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Loop closure"})," happens when a robot revisits an area it has already explored. The VSLAM system recognizes this and ",(0,o.jsx)(n.strong,{children:"corrects all accumulated errors"}),"."]}),"\n",(0,o.jsx)(n.h4,{id:"the-process-1",children:"The Process"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:'Robot explores a building:\n\nFrame 1-50: Explore Room A\n  Position estimates: (0, 0) \u2192 (5, 0) \u2192 (10, 0) ... (50, 0)\n  (odometry drift accumulates)\n\nFrame 51-100: Explore Room B\n  Continues forward, drift increases\n\nFrame 101: Returns to Room A\n\nCamera recognition: "I\'ve seen that corner before! It\'s the corner\nfrom Frame 15"\n\nRealization: "Frame 101 is at same position as Frame 15!"\n\nCorrection: Re-compute all 86 frames in between with correct trajectory\n\nResult: Error reduced from 1 meter to 10 centimeters!\n'})}),"\n",(0,o.jsx)(n.h4,{id:"how-recognition-works",children:"How Recognition Works"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Feature descriptor matching"}),": Extract distinctive features (patterns, colors, corners) and store them"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Bag of Words"}),": Quickly match current image against database of previous images"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Geometric verification"}),": Confirm the match makes geometric sense (features align in 3D)"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"concept-3-map-building",children:"Concept 3: Map Building"}),"\n",(0,o.jsxs)(n.p,{children:["As VSLAM corrects position errors, it simultaneously ",(0,o.jsx)(n.strong,{children:"builds a 3D map"})," of the environment."]}),"\n",(0,o.jsx)(n.h4,{id:"types-of-maps",children:"Types of Maps"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Sparse Map"})," (fast, low memory):"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Only distinctive features stored"}),"\n",(0,o.jsx)(n.li,{children:"Used for localization and planning"}),"\n",(0,o.jsx)(n.li,{children:"Hundreds of features, lightweight"}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Dense Map"})," (slow, high memory):"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Every pixel's 3D position stored"}),"\n",(0,o.jsx)(n.li,{children:"Better for detailed visualization"}),"\n",(0,o.jsx)(n.li,{children:"Megabytes of data"}),"\n"]}),"\n",(0,o.jsx)(n.h4,{id:"the-map-creation-process",children:"The Map Creation Process"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"Input: Camera frames + corrected robot positions\n\u2193\nFor each frame:\n  - Extract 3D coordinates of features\n  - Place them in the map using robot position\n\u2193\nOutput: 3D point cloud representing the environment\n"})}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Result"}),": A robot can explore a room and generate a 3D map without pre-existing information!"]}),"\n",(0,o.jsx)(n.h2,{id:"isaac-ros-vslam-pipeline",children:"Isaac ROS VSLAM Pipeline"}),"\n",(0,o.jsx)(n.p,{children:"NVIDIA's Isaac ROS includes a VSLAM pipeline optimized for humanoid robots:"}),"\n",(0,o.jsx)(n.h3,{id:"pipeline-components",children:"Pipeline Components"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"[Camera Input]\n    \u2193\n[Feature Extraction]\n  \u2193 Detect corners, edges, patterns in image\n    \u2193\n[Feature Matching]\n  \u2193 Find same features in consecutive frames\n    \u2193\n[Odometry Estimation]\n  \u2193 Calculate robot movement from feature motion\n    \u2193\n[Loop Closure Detection]\n  \u2193 Recognize revisited areas\n    \u2193\n[Map Optimization]\n  \u2193 Correct accumulated errors using loop closures\n    \u2193\n[Output]\n  - Estimated robot pose (position + orientation)\n  - Sparse/dense 3D map\n  - Uncertainty estimate\n"})}),"\n",(0,o.jsx)(n.h3,{id:"ros-2-integration",children:"ROS 2 Integration"}),"\n",(0,o.jsx)(n.p,{children:"Isaac ROS VSLAM publishes to standard ROS 2 topics:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"/odom/odometry"}),": Robot pose estimate (X, Y, Z, rotation)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"/odom/map"}),": Current 3D map as point cloud"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"/odom/loop_closure"}),": Signals when loop closure detected"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"Your other ROS 2 nodes (navigation, planning) subscribe to these topics!"}),"\n",(0,o.jsx)(n.h2,{id:"real-world-example-robot-mapping-a-building",children:"Real-World Example: Robot Mapping a Building"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Scenario"}),": Humanoid robot enters a 3-floor office building and must map it."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Process"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:'Floor 1 (Ground Floor):\n  - Robot enters lobby\n  - Camera sees features: glass doors, wooden floor, columns\n  - VSLAM tracks movement through lobby\n  - Robot explores hallway: doors, windows, carpet changes\n  - Odometry drift: 50 meters traveled, 1 meter of error accumulated\n  - VSLAM map: Connected hallway + lobby (~150 stored features)\n\nFloor 2 (Second Floor):\n  - Robot climbs stairs (still tracking features)\n  - New hallway similar to Floor 1 but slightly different\n  - Drift continues: now 1.5 meters error after 100 meters\n\nFloor 3 (Top Floor):\n  - Robot explores, accumulating more drift\n\nReturn Journey - Loop Closure!:\n  - Robot re-enters hallway from Floor 2\n  - Camera matches features with earlier Floor 2 images\n  - VSLAM recognizes: "This is the same hallway!"\n  - Triggers loop closure: corrects all Floor 2-3 trajectory\n  - Error reduced to 20 centimeters!\n\nFinal Result:\n  - 3-floor map with <20cm accuracy\n  - Loop closure triggered 5+ times throughout journey\n  - Confident pose estimate for navigation (next chapter!)\n'})}),"\n",(0,o.jsx)(n.h2,{id:"limitations-of-vslam",children:"Limitations of VSLAM"}),"\n",(0,o.jsx)(n.p,{children:"VSLAM works great but isn't perfect:"}),"\n",(0,o.jsx)(n.h3,{id:"challenge-1-featureless-environments",children:"Challenge 1: Featureless Environments"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Problem"}),": Blank white walls have no distinctive features"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"Frame 1: White wall, no features detected\nFrame 2: White wall, no features detected\n...\nRobot position: Unknown (no features to track)\n"})}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Add artificial markers (QR codes, beacons)"}),"\n",(0,o.jsx)(n.li,{children:"Use texture-rich environments"}),"\n",(0,o.jsx)(n.li,{children:"Combine with wheel odometry or IMU for backup"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"challenge-2-rapid-lighting-changes",children:"Challenge 2: Rapid Lighting Changes"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"Frame 1 (sunny): Bright image, features visible\nFrame 2 (shadow): Dark image, same features hard to detect\nFeature matching fails!\n"})}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Solution"}),": Robust feature detectors trained on diverse lighting."]}),"\n",(0,o.jsx)(n.h3,{id:"challenge-3-dynamic-objects-moving-people",children:"Challenge 3: Dynamic Objects (Moving People)"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"Frame 1: Person at position (5, 5)\nFrame 2: Person moved to position (5, 6)\n\u2193\nVSLAM thinks the room changed, not the person!\n"})}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Solution"}),': Semantic filtering\u2014ignore "people" features, only track static scene.']}),"\n",(0,o.jsx)(n.h3,{id:"challenge-4-computationally-intensive",children:"Challenge 4: Computationally Intensive"}),"\n",(0,o.jsx)(n.p,{children:"Full VSLAM with loop closure requires:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Feature detection: CPU/GPU"}),"\n",(0,o.jsx)(n.li,{children:"Feature matching: CPU/GPU"}),"\n",(0,o.jsx)(n.li,{children:"Optimization: CPU (numerical solvers)"}),"\n",(0,o.jsx)(n.li,{children:"Real-time requirement: must process camera frames at ~30 fps"}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Solution"}),": Optimized implementations (Isaac ROS, ORB-SLAM, etc.) use GPU acceleration."]}),"\n",(0,o.jsx)(n.h2,{id:"comparison-vslam-vs-other-localization-methods",children:"Comparison: VSLAM vs. Other Localization Methods"}),"\n",(0,o.jsxs)(n.table,{children:[(0,o.jsx)(n.thead,{children:(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.th,{children:"Method"}),(0,o.jsx)(n.th,{children:"Requirements"}),(0,o.jsx)(n.th,{children:"Accuracy"}),(0,o.jsx)(n.th,{children:"Cost"}),(0,o.jsx)(n.th,{children:"Use Case"})]})}),(0,o.jsxs)(n.tbody,{children:[(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"GPS"})}),(0,o.jsx)(n.td,{children:"GPS signal, outdoors"}),(0,o.jsx)(n.td,{children:"1-10 meters"}),(0,o.jsx)(n.td,{children:"Free (external)"}),(0,o.jsx)(n.td,{children:"Outdoor robots, drones"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"Wheel Odometry"})}),(0,o.jsx)(n.td,{children:"Wheel encoders"}),(0,o.jsx)(n.td,{children:"Drifts 1m per 100m"}),(0,o.jsx)(n.td,{children:"Cheap"}),(0,o.jsx)(n.td,{children:"Rough estimates"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"Lidar SLAM"})}),(0,o.jsx)(n.td,{children:"Lidar sensor (360\xb0 laser)"}),(0,o.jsx)(n.td,{children:"\xb15cm"}),(0,o.jsx)(n.td,{children:"$500-5000"}),(0,o.jsx)(n.td,{children:"Mapping, navigation"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"VSLAM"})}),(0,o.jsx)(n.td,{children:"Standard camera"}),(0,o.jsx)(n.td,{children:"\xb110cm"}),(0,o.jsx)(n.td,{children:"Free (standard camera)"}),(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"Indoor, lightweight, cheap"})})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"IMU/Gyro"})}),(0,o.jsx)(n.td,{children:"Inertial sensors"}),(0,o.jsx)(n.td,{children:"Drifts quickly"}),(0,o.jsx)(n.td,{children:"Cheap"}),(0,o.jsx)(n.td,{children:"Short-term estimates"})]})]})]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"For humanoid robots"}),": VSLAM is ideal because humanoids already have cameras for perception!"]}),"\n",(0,o.jsx)(n.h2,{id:"what-happens-next",children:"What Happens Next"}),"\n",(0,o.jsx)(n.p,{children:"Now your robot knows:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Where it is"})," (from this chapter: VSLAM)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"The map around it"})," (from this chapter: VSLAM)"]}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:["But knowing where you are doesn't tell you how to reach a goal! In Chapter 4, you'll learn how robots plan collision-free paths using ",(0,o.jsx)(n.strong,{children:"Nav2 Path Planning"}),"."]}),"\n",(0,o.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,o.jsxs)(n.p,{children:["\u2713 ",(0,o.jsx)(n.strong,{children:"Visual Odometry"})," tracks robot movement by detecting feature motion across frames\n\u2713 ",(0,o.jsx)(n.strong,{children:"Loop Closure Detection"})," recognizes revisited areas and corrects accumulated drift\n\u2713 ",(0,o.jsx)(n.strong,{children:"3D Maps"})," are built simultaneously as the robot explores\n\u2713 ",(0,o.jsx)(n.strong,{children:"Isaac ROS VSLAM"})," provides production-ready localization for humanoid robots\n\u2713 ",(0,o.jsx)(n.strong,{children:"VSLAM works with standard cameras"}),", making it cost-effective\n\u2713 ",(0,o.jsx)(n.strong,{children:"Accuracy ~10cm after 100+ meters of exploration"})," enables reliable navigation"]}),"\n",(0,o.jsx)(n.h2,{id:"next-chapter-4",children:"Next: Chapter 4"}),"\n",(0,o.jsxs)(n.p,{children:["Your robot now knows where it is and has a map. How does it get from point A to point B while avoiding obstacles? Find out in ",(0,o.jsx)(n.strong,{children:"Chapter 4: Nav2 Path Planning"}),"."]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Learning Outcome"}),": You now understand how cameras alone enable robots to localize themselves and build maps without GPS, and why VSLAM is critical for indoor robot autonomy."]})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>t,x:()=>l});var s=r(6540);const o={},i=s.createContext(o);function t(e){const n=s.useContext(i);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:t(e.components),s.createElement(i.Provider,{value:n},e.children)}}}]);