"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[7653],{1793:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>h,frontMatter:()=>s,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"module4/ch2-llm-planning","title":"LLM Cognitive Planning","description":"What is an LLM?","source":"@site/docs/module4/chapter2-llm-planning.md","sourceDirName":"module4","slug":"/module4/ch2-llm-planning","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/docs/module4/ch2-llm-planning","draft":false,"unlisted":false,"editUrl":"https://github.com/shabi9999/Physical-AI-Humanoid-Robotics-Textbook/tree/main/my-website/docs/module4/chapter2-llm-planning.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"title":"LLM Cognitive Planning","module":4,"chapter":2,"id":"ch2-llm-planning","sidebar_position":2,"learning_objectives":["Understand how Large Language Models convert text into structured robot plans","Recognize intent extraction, entity identification, and constraint handling mechanisms","Apply prompting techniques to guide LLMs toward robot-compatible output formats"],"prerequisites":["Module 1: ROS 2 Fundamentals completed","Chapter 1: Speech Recognition completed"],"related_chapters":["chapter1-whisper","chapter3-ros2-actions","chapter4-complete-vla"],"keywords":["LLM","intent recognition","semantic understanding","planning","VLA","prompting"],"difficulty":"Beginner","estimated_reading_time":"16 minutes","estimated_word_count":5000,"created_at":"2025-12-08","chunk_count":10,"searchable_terms":["LLM","large language model","intent","entity","semantic","planning","prompt","structured output","few-shot learning","hallucination"]},"sidebar":"docs","previous":{"title":"Speech Recognition with Whisper","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/docs/module4/ch1-whisper"},"next":{"title":"ROS 2 Action Integration","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/docs/module4/ch3-ros2-actions"}}');var o=t(4848),r=t(8453);const s={title:"LLM Cognitive Planning",module:4,chapter:2,id:"ch2-llm-planning",sidebar_position:2,learning_objectives:["Understand how Large Language Models convert text into structured robot plans","Recognize intent extraction, entity identification, and constraint handling mechanisms","Apply prompting techniques to guide LLMs toward robot-compatible output formats"],prerequisites:["Module 1: ROS 2 Fundamentals completed","Chapter 1: Speech Recognition completed"],related_chapters:["chapter1-whisper","chapter3-ros2-actions","chapter4-complete-vla"],keywords:["LLM","intent recognition","semantic understanding","planning","VLA","prompting"],difficulty:"Beginner",estimated_reading_time:"16 minutes",estimated_word_count:5e3,created_at:"2025-12-08",chunk_count:10,searchable_terms:["LLM","large language model","intent","entity","semantic","planning","prompt","structured output","few-shot learning","hallucination"]},l="Chapter 2: LLM Cognitive Planning",a={},c=[{value:"What is an LLM?",id:"what-is-an-llm",level:2},{value:"Key LLMs for Robotics",id:"key-llms-for-robotics",level:3},{value:"How LLMs Extract Intent and Entities",id:"how-llms-extract-intent-and-entities",level:2},{value:"Intent Extraction",id:"intent-extraction",level:3},{value:"Entity Extraction",id:"entity-extraction",level:3},{value:"Prompting: Guiding LLMs Toward Robot Commands",id:"prompting-guiding-llms-toward-robot-commands",level:2},{value:"Basic Prompt",id:"basic-prompt",level:3},{value:"Structured Prompt (Better)",id:"structured-prompt-better",level:3},{value:"Advanced Prompting: Few-Shot Learning",id:"advanced-prompting-few-shot-learning",level:3},{value:"LLM Capabilities for Robots",id:"llm-capabilities-for-robots",level:2},{value:"Strong Capabilities",id:"strong-capabilities",level:3},{value:"Limitations &amp; Failures",id:"limitations--failures",level:3},{value:"Real-World Scenario: Kitchen Robot",id:"real-world-scenario-kitchen-robot",level:2},{value:"User Command",id:"user-command",level:3},{value:"LLM Processing",id:"llm-processing",level:3},{value:"Structured Output",id:"structured-output",level:3},{value:"Robot Response",id:"robot-response",level:3},{value:"Comparison: LLM vs. Traditional Programming",id:"comparison-llm-vs-traditional-programming",level:2},{value:"Traditional Robot Programming",id:"traditional-robot-programming",level:3},{value:"LLM-Based Robot Control",id:"llm-based-robot-control",level:3},{value:"LLM Reasoning: Understanding &quot;Why&quot;",id:"llm-reasoning-understanding-why",level:2},{value:"Integration with Module 1: ROS 2 Coordination",id:"integration-with-module-1-ros-2-coordination",level:2},{value:"Integration in Complete VLA Pipeline",id:"integration-in-complete-vla-pipeline",level:2},{value:"Edge Cases: When LLMs Fail",id:"edge-cases-when-llms-fail",level:2},{value:"Example 1: The Hallucination Problem",id:"example-1-the-hallucination-problem",level:3},{value:"Example 2: Out-of-Domain Commands",id:"example-2-out-of-domain-commands",level:3},{value:"Example 3: Ambiguous Reference",id:"example-3-ambiguous-reference",level:3},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Next: Chapter 3",id:"next-chapter-3",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"chapter-2-llm-cognitive-planning",children:"Chapter 2: LLM Cognitive Planning"})}),"\n",(0,o.jsx)(n.h2,{id:"what-is-an-llm",children:"What is an LLM?"}),"\n",(0,o.jsxs)(n.p,{children:["A ",(0,o.jsx)(n.strong,{children:"Large Language Model (LLM)"})," is an AI system trained on vast amounts of text that learns to predict the next word in a sequence."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Simple analogy"}),": Imagine reading billions of books. You learn:"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"How sentences are structured"}),"\n",(0,o.jsx)(n.li,{children:"What words typically follow each other"}),"\n",(0,o.jsx)(n.li,{children:"How to express ideas clearly"}),"\n",(0,o.jsx)(n.li,{children:"How to answer questions"}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"LLMs do the same with text data!"}),"\n",(0,o.jsx)(n.h3,{id:"key-llms-for-robotics",children:"Key LLMs for Robotics"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"ChatGPT/GPT-4"}),": Conversational, excellent language understanding"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Claude"}),": Strong reasoning, safety-focused"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Llama"}),": Open-source, local deployment possible"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Gemini"}),": Multimodal (text + images), strong reasoning"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"how-llms-extract-intent-and-entities",children:"How LLMs Extract Intent and Entities"}),"\n",(0,o.jsxs)(n.p,{children:["Your robot doesn't need to understand English\u2014it needs to extract ",(0,o.jsx)(n.strong,{children:"what to do"})," and ",(0,o.jsx)(n.strong,{children:"what to act on"}),"."]}),"\n",(0,o.jsx)(n.h3,{id:"intent-extraction",children:"Intent Extraction"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:'User: "Pick up the blue object"\n\nIntent Analysis:\n  - Primary intent: PICK_UP\n  - Modifiers: "gently" (implied by fragility)\n  - Urgency: Normal (no "quickly", "immediately")\n\nRobot understands: Action=PICK_UP with normal force\n'})}),"\n",(0,o.jsx)(n.h3,{id:"entity-extraction",children:"Entity Extraction"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:'User: "Pick up the blue object on the table"\n\nEntities:\n  - Object: "blue object" (color: blue, type: unknown)\n  - Location: "on the table" (where to find it)\n  - Constraint: "gently" (implied, based on fragility)\n\nRobot understands: What=blue_object, Where=table_surface\n'})}),"\n",(0,o.jsx)(n.h2,{id:"prompting-guiding-llms-toward-robot-commands",children:"Prompting: Guiding LLMs Toward Robot Commands"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Prompting"})," is the art of asking LLMs to produce structured output for robots."]}),"\n",(0,o.jsx)(n.h3,{id:"basic-prompt",children:"Basic Prompt"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:'User input: "Pick up the blue object"\n\nNaive prompt:\n"What should the robot do?"\nLLM response: "The robot should pick up a blue object from the table."\nProblem: Still English! Robot can\'t use this.\n'})}),"\n",(0,o.jsx)(n.h3,{id:"structured-prompt-better",children:"Structured Prompt (Better)"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:'Prompt:\n"Extract the robot action from this user command.\nOutput as JSON with fields: action, object, location, constraints.\nUser said: Pick up the blue object"\n\nLLM response:\n{\n  "action": "pick_up",\n  "object": {\n    "color": "blue",\n    "type": "unspecified"\n  },\n  "location": "unspecified",\n  "constraints": {\n    "force": "gentle"\n  }\n}\n'})}),"\n",(0,o.jsxs)(n.p,{children:["\u2705 ",(0,o.jsx)(n.strong,{children:"Structured output"})," that robots can parse!"]}),"\n",(0,o.jsx)(n.h3,{id:"advanced-prompting-few-shot-learning",children:"Advanced Prompting: Few-Shot Learning"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:'Prompt:\n"Extract robot action. Output as JSON.\n\nExamples:\n  Input: \'Place the red cube on the shelf\'\n  Output: {"action": "place", "object": "cube", "color": "red", "location": "shelf"}\n\n  Input: \'Move the robot forward 2 meters\'\n  Output: {"action": "move_forward", "distance": 2.0, "unit": "meters"}\n\nNow process:\n  Input: \'Pick up the blue object carefully\'\n  Output:"\n\nLLM response (from learned pattern):\n{\n  "action": "pick_up",\n  "object": "unknown_type",\n  "color": "blue",\n  "constraints": {"force": "gentle"}\n}\n'})}),"\n",(0,o.jsxs)(n.p,{children:["\u2705 ",(0,o.jsx)(n.strong,{children:"Few examples teach LLM the exact format you want!"})]}),"\n",(0,o.jsx)(n.h2,{id:"llm-capabilities-for-robots",children:"LLM Capabilities for Robots"}),"\n",(0,o.jsx)(n.h3,{id:"strong-capabilities",children:"Strong Capabilities"}),"\n",(0,o.jsxs)(n.table,{children:[(0,o.jsx)(n.thead,{children:(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.th,{children:"Task"}),(0,o.jsx)(n.th,{children:"Example"}),(0,o.jsx)(n.th,{children:"Success Rate"})]})}),(0,o.jsxs)(n.tbody,{children:[(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"Intent extraction"})}),(0,o.jsx)(n.td,{children:'"Pick up" from "Pick up the blue object"'}),(0,o.jsx)(n.td,{children:"98%+"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"Entity identification"})}),(0,o.jsx)(n.td,{children:'"Blue object" is the what, "table" is the where'}),(0,o.jsx)(n.td,{children:"95%+"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"Constraint recognition"})}),(0,o.jsx)(n.td,{children:'"Gently" implies low force'}),(0,o.jsx)(n.td,{children:"90%+"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"Multi-step planning"})}),(0,o.jsx)(n.td,{children:'"Pick up A, move to B, place on C"'}),(0,o.jsx)(n.td,{children:"85%+"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"Clarification"})}),(0,o.jsx)(n.td,{children:'"Which blue object?" when ambiguous'}),(0,o.jsx)(n.td,{children:"90%+"})]})]})]}),"\n",(0,o.jsx)(n.h3,{id:"limitations--failures",children:"Limitations & Failures"}),"\n",(0,o.jsxs)(n.table,{children:[(0,o.jsx)(n.thead,{children:(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.th,{children:"Problem"}),(0,o.jsx)(n.th,{children:"Example"}),(0,o.jsx)(n.th,{children:"Cause"})]})}),(0,o.jsxs)(n.tbody,{children:[(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"Hallucination"})}),(0,o.jsx)(n.td,{children:"Inventing details not in original command"}),(0,o.jsx)(n.td,{children:"LLM generates plausible-sounding but false info"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"Out-of-domain"})}),(0,o.jsx)(n.td,{children:'"Make me a sandwich" (robot has no gripper)'}),(0,o.jsx)(n.td,{children:"LLM doesn't know robot capabilities"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"Ambiguity"})}),(0,o.jsx)(n.td,{children:'"It" without antecedent'}),(0,o.jsx)(n.td,{children:"Requires context"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"Contradiction"})}),(0,o.jsx)(n.td,{children:'"Gently pick it up forcefully"'}),(0,o.jsx)(n.td,{children:"Conflicting instructions"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"Nonsensical"})}),(0,o.jsx)(n.td,{children:'"Move the table to itself"'}),(0,o.jsx)(n.td,{children:"Logically impossible"})]})]})]}),"\n",(0,o.jsx)(n.h2,{id:"real-world-scenario-kitchen-robot",children:"Real-World Scenario: Kitchen Robot"}),"\n",(0,o.jsx)(n.p,{children:"Let's trace LLM understanding in a kitchen context:"}),"\n",(0,o.jsx)(n.h3,{id:"user-command",children:"User Command"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:'"Robot, pour me a glass of water"\n'})}),"\n",(0,o.jsx)(n.h3,{id:"llm-processing",children:"LLM Processing"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"Analysis:\n  - Action: POUR\n  - Object being poured: WATER\n  - Recipient: USER\n  - Container: GLASS\n  - Context: KITCHEN\n\nConstraints inferred from context:\n  - Temperature: ROOM_TEMPERATURE (unless specified)\n  - Quantity: ~250ml (typical glass)\n  - Speed: SLOW (careful pouring)\n"})}),"\n",(0,o.jsx)(n.h3,{id:"structured-output",children:"Structured Output"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-json",children:'{\n  "action": "pour",\n  "object": "water",\n  "recipient": "user",\n  "container": "glass",\n  "quantity_ml": 250,\n  "speed": "slow",\n  "robot_capability_required": "pouring_mechanism"\n}\n'})}),"\n",(0,o.jsx)(n.h3,{id:"robot-response",children:"Robot Response"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"Check capabilities:\n  \u2705 Find water (refrigerator model added to robot's knowledge)\n  \u2705 Locate glass (in kitchen cabinets)\n  \u2705 Pour (requires liquid handling gripper - not available)\n  \u274c FAIL: Robot doesn't have pouring mechanism\n\nRobot to user: \"I don't have a pouring mechanism. Would you like me to bring you the water bottle instead?\"\n"})}),"\n",(0,o.jsx)(n.h2,{id:"comparison-llm-vs-traditional-programming",children:"Comparison: LLM vs. Traditional Programming"}),"\n",(0,o.jsx)(n.h3,{id:"traditional-robot-programming",children:"Traditional Robot Programming"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"def pick_up_blue_object():\n    # Find blue objects in scene\n    blue_objects = vision.find_by_color('blue')\n    if not blue_objects:\n        print(\"No blue objects found\")\n        return False\n\n    # Pick the closest one\n    target = min(blue_objects, key=lambda obj: distance_to(obj))\n\n    # Plan gripper motion\n    trajectory = motion_planner.plan_grasp(target)\n\n    # Execute\n    robot.execute_trajectory(trajectory)\n    return True\n"})}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Problem"}),": Code must be written for every command!"]}),"\n",(0,o.jsx)(n.h3,{id:"llm-based-robot-control",children:"LLM-Based Robot Control"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'user_command = "Pick up the blue object"\n\n# Single LLM call handles variety:\naction_plan = llm.extract_action(user_command)\n# Returns: {"action": "pick_up", "object": {"color": "blue"}}\n\n# Same code executes ANY action:\nexecute_plan(action_plan)\n'})}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Advantage"}),": One LLM call handles endless variations!"]}),"\n",(0,o.jsx)(n.h2,{id:"llm-reasoning-understanding-why",children:'LLM Reasoning: Understanding "Why"'}),"\n",(0,o.jsx)(n.p,{children:"LLMs can reason about context:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:'User: "The bowl is on the table. Pick it up."\n\nLLM reasoning:\n  - "The bowl" refers to bowl mentioned in previous sentence\n  - "it" is pronoun for "bowl"\n  - "on the table" provides location\n  - "Pick it up" is action on the bowl\n\nOutput:\n{\n  "action": "pick_up",\n  "object": "bowl",\n  "location": "table"\n}\n'})}),"\n",(0,o.jsx)(n.p,{children:"vs. Simple rule-based system:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:'Regex: "pick it up"\nAction: "pick_up"\nObject: "it" (Unknown! Pronoun resolution failed)\n'})}),"\n",(0,o.jsx)(n.h2,{id:"integration-with-module-1-ros-2-coordination",children:"Integration with Module 1: ROS 2 Coordination"}),"\n",(0,o.jsxs)(n.p,{children:["The LLM output becomes a ",(0,o.jsx)(n.a,{href:"/module1/ch1-ros2-core",children:"ROS 2 message"})," that coordinates the robot system:"]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Flow"}),":"]}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["Whisper (Chapter 1) publishes text to ",(0,o.jsx)(n.code,{children:"/speech/transcribed"})," topic"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.a,{href:"/module1/ch2-agent-bridge",children:"ROS 2 Python Agent"})," receives transcription"]}),"\n",(0,o.jsx)(n.li,{children:"Agent calls LLM planning service"}),"\n",(0,o.jsx)(n.li,{children:"LLM outputs structured JSON action"}),"\n",(0,o.jsxs)(n.li,{children:["Agent publishes result to ",(0,o.jsx)(n.code,{children:"/planning/action"})," topic"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.a,{href:"/module1/ch1-ros2-core",children:"Action Server"})," receives and executes (Chapter 3)"]}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:["See ",(0,o.jsx)(n.a,{href:"/module1/ch2-agent-bridge",children:"Module 1: Python Agents"})," for detailed coordination patterns and ",(0,o.jsx)(n.a,{href:"/module4/ch3-ros2-actions",children:"Chapter 3: ROS 2 Actions"})," for execution details."]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"integration-in-complete-vla-pipeline",children:"Integration in Complete VLA Pipeline"}),"\n",(0,o.jsx)(n.p,{children:"Here's where LLM fits in the complete system:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:'Chapter 1 (Whisper): Audio \u2192 Text\n    "Pick up the blue object"\n              \u2193\nChapter 2 (LLM): Text \u2192 Structured Plan\n    {action: pick_up, object: blue_object, constraints: {force: gentle}}\n              \u2193\nChapter 3 (ROS 2 Actions): Plan \u2192 Robot Motion\n    Execute trajectory via [Action Servers](/module4/ch3-ros2-actions)\n              \u2193\nChapter 4 (Feedback Loop): Motion \u2192 Perception\n    Confirm object grasped via VSLAM/sensors ([Module 3](/module3/intro))\n'})}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"LLM's role"}),": Translate human language to robot-understandable structured plans"]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"edge-cases-when-llms-fail",children:"Edge Cases: When LLMs Fail"}),"\n",(0,o.jsx)(n.p,{children:"Understanding LLM limitations is critical for robust robot systems:"}),"\n",(0,o.jsx)(n.h3,{id:"example-1-the-hallucination-problem",children:"Example 1: The Hallucination Problem"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:'User: "Get me the xyz object"\n\nLLM might output:\n{\n  "action": "pick_up",\n  "object": "xyz",\n  "color": "red",      // \u2190 Hallucinated! User never said color\n  "size": "small"      // \u2190 Also hallucinated!\n}\n\nRobot searches for "red small xyz object" \u2192 Doesn\'t exist \u2192 Failure\n'})}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Solution"}),": Always validate LLM output against available objects in the scene. See ",(0,o.jsx)(n.a,{href:"/module3/ch1-isaac-sim-fundamentals",children:"Module 3: Vision Systems"})," for object detection integration."]}),"\n",(0,o.jsx)(n.h3,{id:"example-2-out-of-domain-commands",children:"Example 2: Out-of-Domain Commands"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:'User: "Make me a sandwich"\n\nLLM outputs:\n{\n  "action": "make",\n  "object": "sandwich",\n  "ingredients": ["bread", "meat", "cheese"]\n}\n\nRobot reality check:\n- \u2705 Robot has gripper\n- \u274c Robot can\'t use stove\n- \u274c Robot can\'t assemble sandwich\n- \u274c Out of scope for robot capabilities\n\nRobot response: "I can bring you sandwich ingredients, but I can\'t assemble them."\n'})}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Solution"}),": Store robot capability profiles. Validate all LLM outputs against them."]}),"\n",(0,o.jsx)(n.h3,{id:"example-3-ambiguous-reference",children:"Example 3: Ambiguous Reference"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:'User (while pointing): "Pick it up"\n\nLLM gets only text: "Pick it up"\nLLM doesn\'t know what "it" refers to!\n\nWithout context:\n{\n  "action": "pick_up",\n  "object": ???      // \u2190 Undefined!\n}\n'})}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Solution"}),": Pass visual/spatial context to LLM. Include list of visible objects:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:'LLM Prompt:\n"User said: \'Pick it up\'\nVisible objects: [blue_ball, red_cube, green_box]\nWhich object should robot pick up?"\n\nLLM (with context): "The user is probably pointing at the nearest object or most salient one."\n'})}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,o.jsxs)(n.p,{children:["\u2713 ",(0,o.jsx)(n.strong,{children:"LLMs"})," learn language patterns from massive datasets (not programmed explicitly)\n\u2713 ",(0,o.jsx)(n.strong,{children:"Intent extraction"})," identifies what the user wants (pick, place, move, etc.)\n\u2713 ",(0,o.jsx)(n.strong,{children:"Entity extraction"})," identifies what objects and locations are involved\n\u2713 ",(0,o.jsx)(n.strong,{children:"Prompting"})," guides LLMs toward structured, robot-compatible output\n\u2713 ",(0,o.jsx)(n.strong,{children:"Few-shot learning"})," teaches LLM your exact desired output format via examples\n\u2713 ",(0,o.jsx)(n.strong,{children:"Capabilities"}),": Intent, entities, constraints, multi-step planning, reasoning\n\u2713 ",(0,o.jsx)(n.strong,{children:"Limitations"}),": Hallucination, out-of-domain commands, ambiguity without context\n\u2713 ",(0,o.jsx)(n.strong,{children:"Integration"}),": Works within ",(0,o.jsx)(n.a,{href:"/module1/ch1-ros2-core",children:"ROS 2 architecture"})," as a planning service\n\u2713 ",(0,o.jsx)(n.strong,{children:"Advantage over hard-coding"}),": Single LLM handles endless command variations"]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"next-chapter-3",children:"Next: Chapter 3"}),"\n",(0,o.jsxs)(n.p,{children:["Now you have a structured action plan from the LLM. But how does the robot ",(0,o.jsx)(n.strong,{children:"execute"})," it in the real world? In ",(0,o.jsx)(n.strong,{children:(0,o.jsx)(n.a,{href:"/module4/ch3-ros2-actions",children:"Chapter 3: ROS 2 Action Integration"})}),", you'll learn how robots translate plans into actual motion using action servers and trajectory execution."]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Learning Outcome"}),": You now understand how LLMs extract meaning from text, how prompting guides them toward robot commands, why structured output is critical, and why they're more flexible than hard-coded robot programs."]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>l});var i=t(6540);const o={},r=i.createContext(o);function s(e){const n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);