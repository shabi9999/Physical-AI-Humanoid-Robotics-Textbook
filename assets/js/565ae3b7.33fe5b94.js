"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[7377],{8453:(e,n,o)=>{o.d(n,{R:()=>a,x:()=>s});var i=o(6540);const t={},r=i.createContext(t);function a(e){const n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),i.createElement(r.Provider,{value:n},e.children)}},8540:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>p,frontMatter:()=>a,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"module4/ch4-complete-vla","title":"Complete VLA Pipeline","description":"End-to-End Voice Command Execution","source":"@site/docs/module4/chapter4-complete-vla.md","sourceDirName":"module4","slug":"/module4/ch4-complete-vla","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/docs/module4/ch4-complete-vla","draft":false,"unlisted":false,"editUrl":"https://github.com/shabi9999/Physical-AI-Humanoid-Robotics-Textbook/tree/main/my-website/docs/module4/chapter4-complete-vla.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"title":"Complete VLA Pipeline","module":4,"chapter":4,"id":"ch4-complete-vla","sidebar_position":4,"learning_objectives":["Trace complete voice command flow from audio input to physical robot execution","Understand feedback loops and error recovery mechanisms in VLA systems","Recognize how Modules 1-3 integrate to enable intelligent robot behavior"],"prerequisites":["Module 1: ROS 2 Fundamentals completed","Chapters 1-3: Whisper, LLM, ROS 2 Actions completed"],"related_chapters":["chapter1-whisper","chapter2-llm-planning","chapter3-ros2-actions"],"keywords":["VLA","complete pipeline","integration","feedback","end-to-end","voice-to-action"],"difficulty":"Intermediate","estimated_reading_time":"20 minutes","estimated_word_count":5000,"created_at":"2025-12-08","chunk_count":10,"searchable_terms":["VLA","pipeline","voice","action","perception","feedback","integration","error recovery","multi-step commands","complete system"]},"sidebar":"docs","previous":{"title":"ROS 2 Action Integration","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/docs/module4/ch3-ros2-actions"},"next":{"title":"Robotics Glossary & References","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/docs/glossary"}}');var t=o(4848),r=o(8453);const a={title:"Complete VLA Pipeline",module:4,chapter:4,id:"ch4-complete-vla",sidebar_position:4,learning_objectives:["Trace complete voice command flow from audio input to physical robot execution","Understand feedback loops and error recovery mechanisms in VLA systems","Recognize how Modules 1-3 integrate to enable intelligent robot behavior"],prerequisites:["Module 1: ROS 2 Fundamentals completed","Chapters 1-3: Whisper, LLM, ROS 2 Actions completed"],related_chapters:["chapter1-whisper","chapter2-llm-planning","chapter3-ros2-actions"],keywords:["VLA","complete pipeline","integration","feedback","end-to-end","voice-to-action"],difficulty:"Intermediate",estimated_reading_time:"20 minutes",estimated_word_count:5e3,created_at:"2025-12-08",chunk_count:10,searchable_terms:["VLA","pipeline","voice","action","perception","feedback","integration","error recovery","multi-step commands","complete system"]},s="Chapter 4: Complete VLA Pipeline",l={},c=[{value:"End-to-End Voice Command Execution",id:"end-to-end-voice-command-execution",level:2},{value:"Real-World Scenario: Complete Execution",id:"real-world-scenario-complete-execution",level:2},{value:"Phase 1: Voice Input (Whisper)",id:"phase-1-voice-input-whisper",level:3},{value:"Phase 2: Understanding (LLM)",id:"phase-2-understanding-llm",level:3},{value:"Phase 3: Perception (Vision/SLAM)",id:"phase-3-perception-visionslam",level:3},{value:"Phase 4: Planning (Motion Planner)",id:"phase-4-planning-motion-planner",level:3},{value:"Phase 5: Execution (Action Server)",id:"phase-5-execution-action-server",level:3},{value:"Phase 6: Feedback &amp; Confirmation",id:"phase-6-feedback--confirmation",level:3},{value:"Complete VLA Workflow Diagram",id:"complete-vla-workflow-diagram",level:2},{value:"Multi-Step Commands",id:"multi-step-commands",level:2},{value:"Error Recovery",id:"error-recovery",level:2},{value:"Scenario: Gripper Can&#39;t Find Object",id:"scenario-gripper-cant-find-object",level:3},{value:"Scenario: Target Unreachable",id:"scenario-target-unreachable",level:3},{value:"Scenario: Obstacle in Path",id:"scenario-obstacle-in-path",level:3},{value:"VLA in Different Scenarios",id:"vla-in-different-scenarios",level:2},{value:"Scenario A: Kitchen (Clean Structured Environment)",id:"scenario-a-kitchen-clean-structured-environment",level:3},{value:"Scenario B: Warehouse (Cluttered, Technical)",id:"scenario-b-warehouse-cluttered-technical",level:3},{value:"Scenario C: Home (Mixed, Variable)",id:"scenario-c-home-mixed-variable",level:3},{value:"Real-Time Loop",id:"real-time-loop",level:2},{value:"Integration with Modules 1-3",id:"integration-with-modules-1-3",level:2},{value:"Module 1 (ROS 2) Enables VLA",id:"module-1-ros-2-enables-vla",level:3},{value:"Module 3 (Perception) Enhances VLA",id:"module-3-perception-enhances-vla",level:3},{value:"Module 2 (Simulation) Validates VLA",id:"module-2-simulation-validates-vla",level:3},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Congratulations!",id:"congratulations",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"chapter-4-complete-vla-pipeline",children:"Chapter 4: Complete VLA Pipeline"})}),"\n",(0,t.jsx)(n.h2,{id:"end-to-end-voice-command-execution",children:"End-to-End Voice Command Execution"}),"\n",(0,t.jsx)(n.p,{children:"You've learned each component:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Chapter 1"}),": Whisper (audio \u2192 text)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Chapter 2"}),": LLM (text \u2192 action plan)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Chapter 3"}),": Action Server (action plan \u2192 motion)"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Now let's see them work together."}),"\n",(0,t.jsx)(n.h2,{id:"real-world-scenario-complete-execution",children:"Real-World Scenario: Complete Execution"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"User"}),': "Pick up the blue ball on the table"']}),"\n",(0,t.jsx)(n.h3,{id:"phase-1-voice-input-whisper",children:"Phase 1: Voice Input (Whisper)"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'Microphone captures audio\n  \u2193\nWhisper processes: 2 seconds of audio\n  \u2193\nOutput: "Pick up the blue ball on the table"\nConfidence: 98%\n'})}),"\n",(0,t.jsx)(n.h3,{id:"phase-2-understanding-llm",children:"Phase 2: Understanding (LLM)"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'LLM prompt:\n"Extract action, object, location. Output JSON.\nUser said: Pick up the blue ball on the table"\n\nLLM output:\n{\n  "action": "pick_up",\n  "object": {\n    "color": "blue",\n    "type": "ball",\n    "material": "rubber"  // Inferred as soft\n  },\n  "location": {\n    "name": "table",\n    "position": "unknown"  // Depends on camera\n  },\n  "constraints": {\n    "force": "gentle"  // Inferred from material\n  }\n}\n'})}),"\n",(0,t.jsx)(n.h3,{id:"phase-3-perception-visionslam",children:"Phase 3: Perception (Vision/SLAM)"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'Camera or LiDAR scans the room\n  \u2193\nDetects blue ball on table at position (1.2, 0.5, 0.8)m\n  \u2193\nUpdates action plan:\n{\n  "action": "pick_up",\n  "target_position": (1.2, 0.5, 0.8),\n  "gripper_force": 5.0  // Gentle force in Newtons\n}\n'})}),"\n",(0,t.jsx)(n.h3,{id:"phase-4-planning-motion-planner",children:"Phase 4: Planning (Motion Planner)"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"Current robot state:\n  - Gripper at: (0.0, 0.0, 0.0)\n  - Target: (1.2, 0.5, 0.8)\n\nMotion planner computes:\n  - Trajectory avoiding obstacles\n  - Joint angles at each waypoint\n  - Execution time: 3.5 seconds\n"})}),"\n",(0,t.jsx)(n.h3,{id:"phase-5-execution-action-server",children:"Phase 5: Execution (Action Server)"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'Time 0.0s: Start moving\n  Feedback: "Moving to target... 0% progress"\n\nTime 1.2s: Arm extended\n  Feedback: "Moving to target... 35% progress"\n\nTime 2.4s: Approaching target\n  Feedback: "Moving to target... 70% progress"\n\nTime 3.5s: Gripper at target position\n  Feedback: "Closing gripper... 90% progress"\n\nTime 3.7s: Gripper closed\n  Sensors detect object contact\n  Feedback: "Object grasped... 100% progress"\n\nResult: "Success! Blue ball picked up."\n'})}),"\n",(0,t.jsx)(n.h3,{id:"phase-6-feedback--confirmation",children:"Phase 6: Feedback & Confirmation"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"Gripper force sensor: 5.2 N (confirming grasp)\nObject camera: Blue ball confirmed in gripper\n\nSpeech synthesis: \"I've picked up the blue ball. What's next?\"\n"})}),"\n",(0,t.jsx)(n.h2,{id:"complete-vla-workflow-diagram",children:"Complete VLA Workflow Diagram"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                   USER SPEAKS                       \u2502\n\u2502          "Pick up the blue ball"                    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                 \u2502\n                 \u25bc\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502  WHISPER (Speech \u2192 Text)   \u2502\n    \u2502 Chapter 1: Audio to text   \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n             \u2502\n             \u25bc\n    "Pick up the blue ball"\n             \u2502\n             \u25bc\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502  LLM (Text \u2192 Action Plan)          \u2502\n    \u2502  Chapter 2: Language understanding \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n             \u2502\n             \u25bc\n    {action: pick_up, object: blue_ball, ...}\n             \u2502\n             \u25bc\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502  PERCEPTION (Find Target)          \u2502\n    \u2502  Chapter 3 + Cameras/LiDAR         \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n             \u2502\n             \u25bc\n    {action: pick_up, position: (1.2, 0.5, 0.8), ...}\n             \u2502\n             \u25bc\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502  MOTION PLANNER                    \u2502\n    \u2502  Compute trajectory                \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n             \u2502\n             \u25bc\n    Trajectory: [wp0 \u2192 wp1 \u2192 wp2 \u2192 wp3]\n             \u2502\n             \u25bc\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502  ACTION SERVER (Execute Motion)    \u2502\n    \u2502  Chapter 3: ROS 2 Actions          \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n             \u2502\n             \u25bc (with feedback)\n    ROBOT MOVES \u2192 Arm extends \u2192 Gripper closes\n             \u2502\n             \u25bc\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502  PERCEPTION FEEDBACK               \u2502\n    \u2502  Confirm: Object in gripper?       \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n             \u2502\n             \u25bc\n    \u2705 SUCCESS: Object grasped\n             \u2502\n             \u25bc\n    ROBOT SPEAKS: "I\'ve picked up the blue ball"\n'})}),"\n",(0,t.jsx)(n.h2,{id:"multi-step-commands",children:"Multi-Step Commands"}),"\n",(0,t.jsx)(n.p,{children:"VLA can handle complex, multi-step commands:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'User: "Pick up the blue ball, move to the table, and place it down gently"\n\nStep 1: LLM breaks into sequence:\n  1. pick_up(blue_ball)\n  2. move_to(table)\n  3. place_down(gently)\n\nStep 2: Each action executes sequentially:\n  Action 1 result: "Ball picked up"\n    \u2193\n  Action 2 result: "At table"\n    \u2193\n  Action 3 result: "Placed gently"\n\nResult: Complete multi-step task accomplished\n'})}),"\n",(0,t.jsx)(n.h2,{id:"error-recovery",children:"Error Recovery"}),"\n",(0,t.jsx)(n.p,{children:"What if something goes wrong?"}),"\n",(0,t.jsx)(n.h3,{id:"scenario-gripper-cant-find-object",children:"Scenario: Gripper Can't Find Object"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'Step 1: Whisper \u2192 "Pick up the blue ball"\nStep 2: LLM \u2192 {action: pick_up, object: blue_ball}\nStep 3: Perception \u2192 ERROR: No blue ball detected!\n\nRecovery options:\n  1. Ask user: "I don\'t see a blue ball. Can you point to it?"\n  2. Expand search: Look in other areas\n  3. Ask clarification: "Do you mean the blue rubber ball or blue cylinder?"\n\nUser responds: "It\'s on the shelf"\n\nLoop back to Step 3: Perception now finds it on shelf\n'})}),"\n",(0,t.jsx)(n.h3,{id:"scenario-target-unreachable",children:"Scenario: Target Unreachable"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'Step 4: Motion planner \u2192 ERROR: Target position unreachable\n\nRobot\'s maximum reach: 1.5 meters\nTarget position: 2.0 meters away\n\nRecovery:\n  Option 1: Move robot base closer\n  Option 2: Ask user: "The ball is too far. Should I move closer?"\n  Option 3: Suggest alternative: "I can move closer to pick it up"\n'})}),"\n",(0,t.jsx)(n.h3,{id:"scenario-obstacle-in-path",children:"Scenario: Obstacle in Path"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'Mid-execution: Obstacle detected at waypoint 2\n\nRecovery:\n  1. Freeze motion immediately (safety)\n  2. Replan trajectory around obstacle\n  3. Continue execution\n  4. Report: "Obstacle detected, replanning..."\n'})}),"\n",(0,t.jsx)(n.h2,{id:"vla-in-different-scenarios",children:"VLA in Different Scenarios"}),"\n",(0,t.jsx)(n.h3,{id:"scenario-a-kitchen-clean-structured-environment",children:"Scenario A: Kitchen (Clean Structured Environment)"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'User: "Pour me water"\nVLA chain:\n  Whisper: "Pour me water"\n  LLM: {action: pour, target: user, liquid: water}\n  Perception: Find water source, glass, user location\n  Planner: Move to water, grasp, pour, deliver\n  Action: Execute with careful pouring constraint\n  Result: Water delivered to user\n'})}),"\n",(0,t.jsx)(n.h3,{id:"scenario-b-warehouse-cluttered-technical",children:"Scenario B: Warehouse (Cluttered, Technical)"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'User: "Move pallet to zone C"\nVLA chain:\n  Whisper: "Move pallet to zone C"\n  LLM: {action: move, object: pallet, destination: zone_c}\n  Perception: Locate pallet, identify obstacles, confirm zone C\n  Planner: Navigate around obstacles, approach pallet, engage\n  Action: Push pallet to zone C, dock correctly\n  Feedback: "Pallet moved to zone C successfully"\n'})}),"\n",(0,t.jsx)(n.h3,{id:"scenario-c-home-mixed-variable",children:"Scenario C: Home (Mixed, Variable)"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'User: "Tidy up the living room"\nVLA chain:\n  Whisper: "Tidy up the living room"\n  LLM: {action: tidy, location: living_room, strategy: organize}\n  Sub-tasks: pick up toys, arrange cushions, clear floor\n  Each sub-task: Full VLA pipeline\n  Feedback: Progressive updates as rooms tidies\n  Result: "Living room tidied"\n'})}),"\n",(0,t.jsx)(n.h2,{id:"real-time-loop",children:"Real-Time Loop"}),"\n",(0,t.jsx)(n.p,{children:"VLA operates in a control loop:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Wait for user command     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502\n         \u25bc\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502 Parse with Whisper+LLM      \u2502\n    \u2502 (0.5-2 seconds)             \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n             \u2502\n             \u25bc\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502 Execute with Action Servers \u2502\n    \u2502 (varies: 1-30 seconds)      \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n             \u2502\n             \u25bc\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502 Check result                \u2502\n    \u2502 Success? Go to next task    \u2502\n    \u2502 Failure? Recover/ask user   \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n             \u2502\n             \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                \u25bc\n                        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                        \u2502 Repeat          \u2502\n                        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,t.jsx)(n.h2,{id:"integration-with-modules-1-3",children:"Integration with Modules 1-3"}),"\n",(0,t.jsx)(n.h3,{id:"module-1-ros-2-enables-vla",children:"Module 1 (ROS 2) Enables VLA"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"ROS 2 provides:\n  - Nodes: Whisper node, LLM node, Action Server nodes\n  - Topics: Audio topic, text topic, action topic\n  - Services: Vision service for object detection\n  - Actions: arm_controller, gripper_controller\n\nResult: VLA runs on ROS 2 middleware\n"})}),"\n",(0,t.jsx)(n.h3,{id:"module-3-perception-enhances-vla",children:"Module 3 (Perception) Enhances VLA"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'Module 3 provides:\n  - VSLAM: Robot knows where it is (for planning)\n  - LIDAR: 360\xb0 obstacle detection\n  - Depth camera: Object detection and grasping\n\nFeedback loop:\n  LLM says: "Pick up blue object"\n  VSLAM says: "Robot at (5, 5), blue object at (7, 7)"\n  Nav2 says: "Path is clear, execute"\n  Result: Confident execution\n'})}),"\n",(0,t.jsx)(n.h3,{id:"module-2-simulation-validates-vla",children:"Module 2 (Simulation) Validates VLA"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"Before deploying on real robot:\n  1. Test VLA system in Gazebo\n  2. Add realistic noise to Whisper input\n  3. Test LLM understanding with ambiguous commands\n  4. Validate Action Server execution\n  5. Confirm feedback loop works\n\nResult: Robust VLA ready for real hardware\n"})}),"\n",(0,t.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,t.jsxs)(n.p,{children:["\u2713 ",(0,t.jsx)(n.strong,{children:"VLA pipeline"})," chains Whisper \u2192 LLM \u2192 Planner \u2192 Action Server\n\u2713 ",(0,t.jsx)(n.strong,{children:"End-to-end"})," from voice to motion execution\n\u2713 ",(0,t.jsx)(n.strong,{children:"Feedback loops"})," enable confirmation and error recovery\n\u2713 ",(0,t.jsx)(n.strong,{children:"Multi-step commands"})," handled by breaking into subtasks\n\u2713 ",(0,t.jsx)(n.strong,{children:"Error recovery"})," with user interaction when needed\n\u2713 ",(0,t.jsx)(n.strong,{children:"Real-time operation"})," with fast response times\n\u2713 ",(0,t.jsx)(n.strong,{children:"Modules 1-3"})," provide supporting infrastructure"]}),"\n",(0,t.jsx)(n.h2,{id:"congratulations",children:"Congratulations!"}),"\n",(0,t.jsx)(n.p,{children:"You've completed Module 4! You now understand how humanoid robots:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\u2705 ",(0,t.jsx)(n.strong,{children:"Hear"})," (Whisper)"]}),"\n",(0,t.jsxs)(n.li,{children:["\u2705 ",(0,t.jsx)(n.strong,{children:"Understand"})," (LLM)"]}),"\n",(0,t.jsxs)(n.li,{children:["\u2705 ",(0,t.jsx)(n.strong,{children:"Plan"})," (Motion Planner)"]}),"\n",(0,t.jsxs)(n.li,{children:["\u2705 ",(0,t.jsx)(n.strong,{children:"Execute"})," (Action Servers)"]}),"\n",(0,t.jsxs)(n.li,{children:["\u2705 ",(0,t.jsx)(n.strong,{children:"Learn from feedback"})," (Perception loops)"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,t.jsx)(n.p,{children:"Future modules will teach you to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Implement"})," a complete VLA system with real code"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Deploy"})," on actual humanoid robots"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Optimize"})," for speed, accuracy, safety"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Extend"})," with multimodal perception (vision + language)"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["You're now a ",(0,t.jsx)(n.strong,{children:"VLA system expert"}),"!"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Learning Outcome"}),": You now understand how all components (Whisper, LLM, Motion Planning, Action Servers) work together to enable voice-controlled humanoid robots."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Congratulations on completing Module 4!"})," \ud83c\udf89"]}),"\n",(0,t.jsx)(n.p,{children:"You've learned how robots understand and act on human voice commands. This is a core capability for human-robot interaction and autonomous systems!"})]})}function p(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}}}]);