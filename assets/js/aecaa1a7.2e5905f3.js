"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[1515],{8453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>o});var s=i(6540);const r={},t=s.createContext(r);function a(e){const n=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:a(e.components),s.createElement(t.Provider,{value:n},e.children)}},9135:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>a,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"module2/ch4-sensor-simulation","title":"Chapter 4: Sensor Simulation","description":"Why Simulate Sensors?","source":"@site/docs/module2/chapter4-sensor-simulation.md","sourceDirName":"module2","slug":"/module2/ch4-sensor-simulation","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/docs/module2/ch4-sensor-simulation","draft":false,"unlisted":false,"editUrl":"https://github.com/shabi9999/Physical-AI-Humanoid-Robotics-Textbook/tree/main/my-website/docs/module2/chapter4-sensor-simulation.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4,"title":"Chapter 4: Sensor Simulation","module":2,"chapter":4,"id":"ch4-sensor-simulation","learning_objectives":["Understand how to simulate realistic sensors (LiDAR, cameras, IMU)","Configure sensor parameters for accurate perception","Generate synthetic training data for ML models"],"prerequisites":["Chapters 1-3: Digital Twin, Physics, World Building","Module 1: ROS 2 Fundamentals"],"related_chapters":["chapter1-digital-twin-concepts","chapter2-gazebo-physics","chapter3-world-building","chapter5-unity-visualization"],"keywords":["sensor simulation","LiDAR","depth camera","IMU","point cloud"],"difficulty":"Intermediate-Advanced","estimated_reading_time":"24 minutes","estimated_word_count":3200,"created_at":"2025-12-09","chunk_count":7,"searchable_terms":["sensor simulation","LiDAR","camera","IMU","point cloud"]},"sidebar":"docs","previous":{"title":"Chapter 3: Building Custom Worlds","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/docs/module2/ch3-world-building"},"next":{"title":"Chapter 5: Unity Visualization & Human-Robot Interaction","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/docs/module2/ch5-unity-visualization"}}');var r=i(4848),t=i(8453);const a={sidebar_position:4,title:"Chapter 4: Sensor Simulation",module:2,chapter:4,id:"ch4-sensor-simulation",learning_objectives:["Understand how to simulate realistic sensors (LiDAR, cameras, IMU)","Configure sensor parameters for accurate perception","Generate synthetic training data for ML models"],prerequisites:["Chapters 1-3: Digital Twin, Physics, World Building","Module 1: ROS 2 Fundamentals"],related_chapters:["chapter1-digital-twin-concepts","chapter2-gazebo-physics","chapter3-world-building","chapter5-unity-visualization"],keywords:["sensor simulation","LiDAR","depth camera","IMU","point cloud"],difficulty:"Intermediate-Advanced",estimated_reading_time:"24 minutes",estimated_word_count:3200,created_at:"2025-12-09",chunk_count:7,searchable_terms:["sensor simulation","LiDAR","camera","IMU","point cloud"]},o="Chapter 4: Sensor Simulation",l={},d=[{value:"Why Simulate Sensors?",id:"why-simulate-sensors",level:2},{value:"LiDAR Simulation",id:"lidar-simulation",level:2},{value:"What is LiDAR?",id:"what-is-lidar",level:3},{value:"Attaching LiDAR to Robot",id:"attaching-lidar-to-robot",level:3},{value:"LiDAR Output in ROS 2",id:"lidar-output-in-ros-2",level:3},{value:"Configuring Realistic Noise",id:"configuring-realistic-noise",level:3},{value:"Depth Camera Simulation",id:"depth-camera-simulation",level:2},{value:"What is a Depth Camera?",id:"what-is-a-depth-camera",level:3},{value:"Attaching Depth Camera",id:"attaching-depth-camera",level:3},{value:"Depth Camera Output in ROS 2",id:"depth-camera-output-in-ros-2",level:3},{value:"IMU Simulation",id:"imu-simulation",level:2},{value:"What is an IMU?",id:"what-is-an-imu",level:3},{value:"Attaching IMU",id:"attaching-imu",level:3},{value:"IMU Output in ROS 2",id:"imu-output-in-ros-2",level:3},{value:"Sensor Validation: Real vs. Simulated",id:"sensor-validation-real-vs-simulated",level:2},{value:"LiDAR Validation Example",id:"lidar-validation-example",level:3},{value:"Depth Camera Validation",id:"depth-camera-validation",level:3},{value:"Sensor Simulation Pipeline Diagram",id:"sensor-simulation-pipeline-diagram",level:3},{value:"Common Sensor Configuration Mistakes",id:"common-sensor-configuration-mistakes",level:2},{value:"Mistake 1: Wrong Sensor Pose",id:"mistake-1-wrong-sensor-pose",level:3},{value:"Mistake 2: Too Much Noise",id:"mistake-2-too-much-noise",level:3},{value:"Mistake 3: Too Fast Update Rate",id:"mistake-3-too-fast-update-rate",level:3},{value:"Real-World Scenario: Robot Grasping",id:"real-world-scenario-robot-grasping",level:2},{value:"Setup",id:"setup",level:3},{value:"Sensor Input",id:"sensor-input",level:3},{value:"Control Response",id:"control-response",level:3},{value:"Realistic vs. Unrealistic",id:"realistic-vs-unrealistic",level:3},{value:"Cross-Module Connections",id:"cross-module-connections",level:2},{value:"Sensor Fusion",id:"sensor-fusion",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Acronym Reference",id:"acronym-reference",level:2},{value:"Next: Chapter 5",id:"next-chapter-5",level:2}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"chapter-4-sensor-simulation",children:"Chapter 4: Sensor Simulation"})}),"\n",(0,r.jsx)(n.h2,{id:"why-simulate-sensors",children:"Why Simulate Sensors?"}),"\n",(0,r.jsxs)(n.p,{children:["Your robot perceives the world through ",(0,r.jsx)(n.strong,{children:"sensors"}),". Realistic sensor simulation means:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Your perception algorithms work on real robots (sim-to-real transfer)"}),"\n",(0,r.jsx)(n.li,{children:"You can test sensor failure modes safely"}),"\n",(0,r.jsx)(n.li,{children:"You can generate training data for machine learning"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"In this chapter, you'll learn to attach and configure three critical sensors:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"LiDAR"})," (Light Detection and Ranging): Laser range finder (point cloud)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Depth Camera"}),": RGB-D sensor (color + distance)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"IMU"})," (Inertial Measurement Unit): Measures acceleration and rotation"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"lidar-simulation",children:"LiDAR Simulation"}),"\n",(0,r.jsx)(n.h3,{id:"what-is-lidar",children:"What is LiDAR?"}),"\n",(0,r.jsx)(n.p,{children:"LiDAR emits laser pulses and measures time-to-reflection:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Outputs: ",(0,r.jsx)(n.strong,{children:"Point cloud"})," (array of (x, y, z) coordinates)"]}),"\n",(0,r.jsx)(n.li,{children:"Range: Typically 5-30 meters"}),"\n",(0,r.jsx)(n.li,{children:"Resolution: Hundreds to millions of points per scan"}),"\n",(0,r.jsx)(n.li,{children:"Use: Obstacle detection, SLAM, mapping"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"attaching-lidar-to-robot",children:"Attaching LiDAR to Robot"}),"\n",(0,r.jsx)(n.p,{children:"In your robot's URDF, add a sensor:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-xml",children:'<link name="lidar_mount">\n  <inertial>\n    <mass>0.1</mass>\n    <inertia>\n      <ixx>0.001</ixx>\n      <iyy>0.001</iyy>\n      <izz>0.001</izz>\n    </inertia>\n  </inertial>\n  \x3c!-- Visual: show where sensor is mounted --\x3e\n  <visual name="visual">\n    <geometry>\n      <cylinder>\n        <radius>0.05</radius>\n        <length>0.1</length>\n      </cylinder>\n    </geometry>\n  </visual>\n  \x3c!-- Collision: don\'t collide with world --\x3e\n  <collision name="collision">\n    <geometry>\n      <cylinder>\n        <radius>0.05</radius>\n        <length>0.1</length>\n      </cylinder>\n    </geometry>\n  </collision>\n  \x3c!-- Sensor specification --\x3e\n  <sensor name="lidar" type="gpu_lidar">\n    <pose>0 0 0.05 0 0 0</pose>\n    <update_rate>10</update_rate>\n    <lidar>\n      <scan>\n        <horizontal>\n          <samples>640</samples>\n          <resolution>1</resolution>\n          <min_angle>-3.14159</min_angle>\n          <max_angle>3.14159</max_angle>\n        </horizontal>\n        <vertical>\n          <samples>32</samples>\n          <resolution>1</resolution>\n          <min_angle>-0.5236</min_angle>\n          <max_angle>0.5236</max_angle>\n        </vertical>\n      </scan>\n      <range>\n        <min>0.3</min>\n        <max>30</max>\n        <resolution>0.02</resolution>\n      </range>\n      <noise>\n        <type>gaussian</type>\n        <mean>0</mean>\n        <stddev>0.01</stddev>\n      </noise>\n    </lidar>\n  </sensor>\n</link>\n'})}),"\n",(0,r.jsx)(n.h3,{id:"lidar-output-in-ros-2",children:"LiDAR Output in ROS 2"}),"\n",(0,r.jsx)(n.p,{children:"Gazebo publishes LiDAR data to ROS 2:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Topic: /lidar/points\nMessage type: sensor_msgs/PointCloud2\n\nData: 3D point coordinates (x, y, z) for each return\n"})}),"\n",(0,r.jsx)(n.p,{children:"Visualize in RViz:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"rviz2\n# Add PointCloud2 display\n# Set topic to /lidar/points\n"})}),"\n",(0,r.jsx)(n.h3,{id:"configuring-realistic-noise",children:"Configuring Realistic Noise"}),"\n",(0,r.jsx)(n.p,{children:"Real LiDAR has noise. Configure it:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-xml",children:"<noise>\n  <type>gaussian</type>\n  <mean>0.0</mean>      \x3c!-- Unbiased --\x3e\n  <stddev>0.01</stddev> \x3c!-- 1cm standard deviation --\x3e\n</noise>\n"})}),"\n",(0,r.jsx)(n.p,{children:"Different sensors have different noise levels:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Sick LiDAR"}),": ~2cm error"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Velodyne"}),": ~5cm error"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Budget LiDAR"}),": ~10cm error"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"depth-camera-simulation",children:"Depth Camera Simulation"}),"\n",(0,r.jsx)(n.h3,{id:"what-is-a-depth-camera",children:"What is a Depth Camera?"}),"\n",(0,r.jsx)(n.p,{children:"Depth camera outputs two images:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"RGB image"}),": Regular color photo"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Depth map"}),": Distance to each pixel"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Uses: Object detection, grasping, hand-eye coordination"}),"\n",(0,r.jsx)(n.h3,{id:"attaching-depth-camera",children:"Attaching Depth Camera"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-xml",children:'<link name="camera">\n  <inertial>\n    <mass>0.05</mass>\n    <inertia>\n      <ixx>0.0001</ixx>\n      <iyy>0.0001</iyy>\n      <izz>0.0001</izz>\n    </inertia>\n  </inertial>\n  \x3c!-- Visual representation --\x3e\n  <visual name="visual">\n    <geometry>\n      <box>\n        <size>0.05 0.05 0.05</size>\n      </box>\n    </geometry>\n  </visual>\n  \x3c!-- Sensor configuration --\x3e\n  <sensor name="depth_camera" type="depth_camera">\n    <pose>0 0 0 0 1.5708 0</pose>  \x3c!-- Looking forward --\x3e\n    <update_rate>30</update_rate>\n    <camera>\n      <horizontal_fov>1.0472</horizontal_fov>  \x3c!-- 60 degrees --\x3e\n      <image>\n        <width>640</width>\n        <height>480</height>\n        <format>R8G8B8</format>\n      </image>\n      <clip>\n        <near>0.1</near>\n        <far>10.0</far>\n      </clip>\n      <noise>\n        <type>gaussian</type>\n        <mean>0</mean>\n        <stddev>0.001</stddev>\n      </noise>\n    </camera>\n  </sensor>\n</link>\n'})}),"\n",(0,r.jsx)(n.h3,{id:"depth-camera-output-in-ros-2",children:"Depth Camera Output in ROS 2"}),"\n",(0,r.jsx)(n.p,{children:"Gazebo publishes depth camera to ROS 2:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Topics:\n  /camera/color/image_raw       (RGB image)\n  /camera/depth/image_rect_raw  (Depth map)\n  /camera/camera_info           (Calibration)\n"})}),"\n",(0,r.jsx)(n.p,{children:"Example: Use depth for grasping"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import cv2\ndepth_image = receive_ros_message('/camera/depth/image_rect_raw')\n# Find objects (depth < 1.0 meters)\nclose_objects = depth_image[depth_image < 1.0]\n# Plan grasp\n"})}),"\n",(0,r.jsx)(n.h2,{id:"imu-simulation",children:"IMU Simulation"}),"\n",(0,r.jsx)(n.h3,{id:"what-is-an-imu",children:"What is an IMU?"}),"\n",(0,r.jsx)(n.p,{children:"IMU (Inertial Measurement Unit) measures:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Acceleration"})," (3 axes): How fast moving"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Angular velocity"})," (3 axes): How fast rotating"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Magnetic field"})," (optional): Compass heading"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Uses: Balance control, motion detection, odometry"}),"\n",(0,r.jsx)(n.h3,{id:"attaching-imu",children:"Attaching IMU"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-xml",children:'<link name="imu_link">\n  <inertial>\n    <mass>0.01</mass>\n    <inertia>\n      <ixx>0.00001</ixx>\n      <iyy>0.00001</iyy>\n      <izz>0.00001</izz>\n    </inertia>\n  </inertial>\n  <sensor name="imu" type="imu">\n    <pose>0 0 0 0 0 0</pose>\n    <update_rate>100</update_rate>\n    <imu>\n      <angular_velocity>\n        <x>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>0.001</stddev>\n          </noise>\n        </x>\n        <y>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>0.001</stddev>\n          </noise>\n        </y>\n        <z>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>0.001</stddev>\n          </noise>\n        </z>\n      </angular_velocity>\n      <linear_acceleration>\n        <x>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>0.01</stddev>\n          </noise>\n        </x>\n        <y>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>0.01</stddev>\n          </noise>\n        </y>\n        <z>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>0.01</stddev>\n          </noise>\n        </z>\n      </linear_acceleration>\n    </imu>\n  </sensor>\n</link>\n'})}),"\n",(0,r.jsx)(n.h3,{id:"imu-output-in-ros-2",children:"IMU Output in ROS 2"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Topic: /imu/data\nMessage type: sensor_msgs/Imu\n\nData:\n  angular_velocity: (x, y, z) rad/s\n  linear_acceleration: (x, y, z) m/s\xb2\n  orientation: (quaternion) from integration\n"})}),"\n",(0,r.jsx)(n.h2,{id:"sensor-validation-real-vs-simulated",children:"Sensor Validation: Real vs. Simulated"}),"\n",(0,r.jsx)(n.h3,{id:"lidar-validation-example",children:"LiDAR Validation Example"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Real sensor"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Scan distance: 5.234 meters\nScan distance: 5.231 meters\nScan distance: 5.238 meters\nAverage: 5.23 \xb1 0.04 meters\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simulated sensor"})," (configured correctly):"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Scan distance: 5.234 meters (same object)\nScan distance: 5.231 meters\nScan distance: 5.239 meters\nAverage: 5.23 \xb1 0.04 meters\n"})}),"\n",(0,r.jsxs)(n.p,{children:["\u2705 ",(0,r.jsx)(n.strong,{children:"Matches!"})," Simulation is realistic."]}),"\n",(0,r.jsx)(n.h3,{id:"depth-camera-validation",children:"Depth Camera Validation"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Real camera"})," (RealSense):"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Pixel at (320, 240): depth = 0.823 meters\nPixel at (321, 240): depth = 0.824 meters\nPixel at (320, 241): depth = 0.824 meters\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simulated camera"})," (configured correctly):"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Same scene, same position: depth \u2248 0.823 \xb1 0.002 meters\n"})}),"\n",(0,r.jsxs)(n.p,{children:["\u2705 ",(0,r.jsx)(n.strong,{children:"Matches!"})," Simulation is realistic."]}),"\n",(0,r.jsx)(n.h3,{id:"sensor-simulation-pipeline-diagram",children:"Sensor Simulation Pipeline Diagram"}),"\n",(0,r.jsx)(n.mermaid,{value:'graph LR\n    A["Robot in\\nGazebo World"] --\x3e B["Sensor\\nSimulation"]\n\n    B --\x3e C["LiDAR\\nRay-casting"]\n    B --\x3e D["Depth Camera\\nRendering"]\n    B --\x3e E["IMU\\nDifferentiation"]\n\n    C --\x3e F["Point Cloud\\n/lidar/points"]\n    D --\x3e G["RGB + Depth\\n/camera/depth"]\n    E --\x3e H["6-axis IMU\\n/imu/data"]\n\n    F --\x3e I["ROS 2 Topics"]\n    G --\x3e I\n    H --\x3e I\n\n    I --\x3e J["Perception\\nAlgorithms"]\n    J --\x3e K["Control\\nDecisions"]\n\n    L["Realistic Noise"] -.->|Applied to| C\n    L -.->|Applied to| D\n    L -.->|Applied to| E\n\n    style A fill:#ff9999\n    style B fill:#ffcc99\n    style C fill:#99ccff\n    style D fill:#99ccff\n    style E fill:#99ccff\n    style I fill:#ff99cc\n    style J fill:#99ff99'}),"\n",(0,r.jsx)(n.h2,{id:"common-sensor-configuration-mistakes",children:"Common Sensor Configuration Mistakes"}),"\n",(0,r.jsx)(n.h3,{id:"mistake-1-wrong-sensor-pose",children:"Mistake 1: Wrong Sensor Pose"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Wrong"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-xml",children:"<pose>0 0 0 0 0 0</pose>  \x3c!-- Sensor at origin of robot! --\x3e\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Correct"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-xml",children:"<pose>0.1 0 1.5 0 1.5708 0</pose>  \x3c!-- Head-mounted camera, looking forward --\x3e\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Result"}),": Wrong sensor position \u2192 perception failures"]}),"\n",(0,r.jsx)(n.h3,{id:"mistake-2-too-much-noise",children:"Mistake 2: Too Much Noise"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Wrong"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-xml",children:"<stddev>1.0</stddev>  \x3c!-- 1 meter noise on rangefinder! --\x3e\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Correct"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-xml",children:"<stddev>0.01</stddev>  \x3c!-- 1cm noise --\x3e\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Result"}),": Sensors unusable for perception"]}),"\n",(0,r.jsx)(n.h3,{id:"mistake-3-too-fast-update-rate",children:"Mistake 3: Too Fast Update Rate"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Wrong"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-xml",children:"<update_rate>1000</update_rate>  \x3c!-- 1000 Hz! Too slow and unrealistic --\x3e\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Correct"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-xml",children:"<update_rate>30</update_rate>  \x3c!-- 30 Hz is typical for depth camera --\x3e\n<update_rate>10</update_rate>  \x3c!-- 10 Hz is typical for LiDAR --\x3e\n<update_rate>100</update_rate> \x3c!-- 100 Hz is typical for IMU --\x3e\n"})}),"\n",(0,r.jsx)(n.h2,{id:"real-world-scenario-robot-grasping",children:"Real-World Scenario: Robot Grasping"}),"\n",(0,r.jsx)(n.p,{children:"Let's trace how sensors enable a robot to grasp objects:"}),"\n",(0,r.jsx)(n.h3,{id:"setup",children:"Setup"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"- Humanoid robot with depth camera on gripper\n- Object (box) on table\n- Distance to box: 0.5 meters\n"})}),"\n",(0,r.jsx)(n.h3,{id:"sensor-input",children:"Sensor Input"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Depth camera reads:\n  - Pixels at (320\xb150, 240\xb150): depth = 0.50 \xb1 0.01 meters\n  - Object detected! At position (0.5, 0, 0.5) relative to gripper\n"})}),"\n",(0,r.jsx)(n.h3,{id:"control-response",children:"Control Response"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"depth = receive_depth_image()\nobject_distance = analyze_depth(depth)  # 0.50 meters\n\nif object_distance < 0.6:  # Object reachable\n    close_gripper()\n    feedback = gripper_force()\n    if feedback > 2.0:  # Holding something\n        retract_gripper()\n        move_to_dropoff()\n"})}),"\n",(0,r.jsx)(n.h3,{id:"realistic-vs-unrealistic",children:"Realistic vs. Unrealistic"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Realistic"})," (with noise):"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Sensor noise occasionally reports 0.48-0.52 meters\nControl adapts: still successful grasp\nRobot learns to be robust\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Unrealistic"})," (perfect sensor):"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Always exactly 0.50 meters\nControl too brittle\nFails on real robot (which has noise)\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Better"}),": Configure realistic noise during simulation!"]}),"\n",(0,r.jsx)(n.h2,{id:"cross-module-connections",children:"Cross-Module Connections"}),"\n",(0,r.jsx)(n.p,{children:"Sensor simulation is critical for downstream perception and learning:"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"From Module 1 (ROS 2 Fundamentals)"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"ROS 2 sensor nodes"})," (Module 1, Chapter 1) publish LiDAR, camera, and IMU data"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Autonomous agents"})," (Module 1, Chapter 2) subscribe to sensor topics for decision-making"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"URDF sensor mounts"})," (Module 1, Chapter 3) determine sensor placement and field of view"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"To Module 3 (Isaac Sim & Perception)"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Synthetic sensor data"})," (this chapter) trains ",(0,r.jsx)(n.strong,{children:"VSLAM algorithms"})," in Module 3"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"LiDAR point clouds"})," enable ",(0,r.jsx)(n.strong,{children:"3D mapping and localization"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Depth camera data"})," enables ",(0,r.jsx)(n.strong,{children:"object detection and manipulation"})," in Module 3"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"To Module 4 (VLA Pipeline)"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Multimodal sensor fusion"})," (this chapter) feeds into language understanding"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Camera feeds"})," enable ",(0,r.jsx)(n.strong,{children:"visual question answering"})," in Module 4"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Sensor noise simulation"})," teaches robots to be robust to imperfect perception"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"sensor-fusion",children:"Sensor Fusion"}),"\n",(0,r.jsx)(n.p,{children:"Real robots combine multiple sensors:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"LiDAR data:\n  - Wide field of view\n  - Good for obstacle detection\n  - Works day/night\n\nDepth camera:\n  - Narrow field of view\n  - Good for manipulation\n  - Requires light\n\nIMU:\n  - Proprioception\n  - Balance feedback\n  - Very reliable\n\nFusion:\n  Use all three together!\n  - LiDAR for navigation obstacles\n  - Depth camera for grasping\n  - IMU for balance control\n"})}),"\n",(0,r.jsx)(n.p,{children:"Gazebo simulates all three, enabling sensor fusion development."}),"\n",(0,r.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,r.jsxs)(n.p,{children:["\u2713 ",(0,r.jsx)(n.strong,{children:"LiDAR"})," simulates point clouds for obstacle detection\n\u2713 ",(0,r.jsx)(n.strong,{children:"Depth camera"})," simulates RGB + depth for manipulation\n\u2713 ",(0,r.jsx)(n.strong,{children:"IMU"})," simulates acceleration and angular velocity for balance\n\u2713 ",(0,r.jsx)(n.strong,{children:"Realistic noise"})," is critical for sim-to-real transfer\n\u2713 ",(0,r.jsx)(n.strong,{children:"Sensor placement"})," (pose) must match your robot\n\u2713 ",(0,r.jsx)(n.strong,{children:"Validation"})," by comparing simulated vs. real measurements\n\u2713 ",(0,r.jsx)(n.strong,{children:"Sensor fusion"})," combines multiple sensors for robust perception"]}),"\n",(0,r.jsx)(n.h2,{id:"acronym-reference",children:"Acronym Reference"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Acronym"}),(0,r.jsx)(n.th,{children:"Full Name"}),(0,r.jsx)(n.th,{children:"Definition"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"LiDAR"})}),(0,r.jsx)(n.td,{children:"Light Detection and Ranging"}),(0,r.jsx)(n.td,{children:"Laser sensor measuring distance via time-of-flight (outputs point cloud)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Depth"})}),(0,r.jsx)(n.td,{children:"Depth Sensing"}),(0,r.jsx)(n.td,{children:"Camera technology measuring distance to objects (RGB-D)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Camera"})}),(0,r.jsx)(n.td,{children:"Image Sensor"}),(0,r.jsx)(n.td,{children:"Optics and sensor capturing visual information (RGB or depth)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"IMU"})}),(0,r.jsx)(n.td,{children:"Inertial Measurement Unit"}),(0,r.jsx)(n.td,{children:"Sensor measuring acceleration and angular velocity (6-axis)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Point Cloud"})}),(0,r.jsx)(n.td,{children:"3D Point Data"}),(0,r.jsx)(n.td,{children:"Unstructured collection of 3D points representing scanned geometry"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Sensor"})}),(0,r.jsx)(n.td,{children:"Measurement Device"}),(0,r.jsx)(n.td,{children:"Physical or simulated device that observes environment properties"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Simulation"})}),(0,r.jsx)(n.td,{children:"Virtual Environment"}),(0,r.jsx)(n.td,{children:"Computer model of robot and world for testing behaviors"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Noise"})}),(0,r.jsx)(n.td,{children:"Measurement Error"}),(0,r.jsx)(n.td,{children:"Gaussian or systematic error added to sensor readings"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Calibration"})}),(0,r.jsx)(n.td,{children:"Sensor Tuning"}),(0,r.jsx)(n.td,{children:"Process of adjusting sensor parameters to match reality"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Range"})}),(0,r.jsx)(n.td,{children:"Measurement Distance"}),(0,r.jsx)(n.td,{children:"Minimum and maximum distances sensor can detect"})]})]})]}),"\n",(0,r.jsx)(n.h2,{id:"next-chapter-5",children:"Next: Chapter 5"}),"\n",(0,r.jsxs)(n.p,{children:["Your robot now perceives the simulated world through realistic sensors. But how do you ",(0,r.jsx)(n.strong,{children:"visualize"})," what's happening? In ",(0,r.jsx)(n.strong,{children:"Chapter 5: Unity Visualization"}),", you'll add photorealistic rendering and enable human-robot interaction scenarios."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Learning Outcome"}),": You can now attach and configure realistic sensors (LiDAR, depth camera, IMU) in Gazebo, validate their accuracy, and use sensor data for robot perception tasks."]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}}}]);