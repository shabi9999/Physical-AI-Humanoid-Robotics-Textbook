"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[9813],{3647:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>l,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"module4/ch1-whisper","title":"Speech Recognition with Whisper","description":"What is Speech Recognition?","source":"@site/docs/module4/chapter1-whisper-speech.md","sourceDirName":"module4","slug":"/module4/ch1-whisper","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/docs/module4/ch1-whisper","draft":false,"unlisted":false,"editUrl":"https://github.com/shabi9999/Physical-AI-Humanoid-Robotics-Textbook/tree/main/my-website/docs/module4/chapter1-whisper-speech.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"title":"Speech Recognition with Whisper","module":4,"chapter":1,"id":"ch1-whisper","sidebar_position":1,"learning_objectives":["Explain Whisper\'s role in the VLA pipeline as the voice-to-text entry point","Understand multilingual speech recognition capabilities, noise robustness, and limitations","Recognize why transcription alone is insufficient for robot understanding and control"],"prerequisites":["Module 1: ROS 2 Fundamentals completed"],"related_chapters":["chapter2-llm-planning","chapter4-complete-vla"],"keywords":["speech recognition","Whisper","audio transcription","VLA pipeline","multilingual support","noise robustness"],"difficulty":"Beginner","estimated_reading_time":"15 minutes","estimated_word_count":5000,"created_at":"2025-12-08","chunk_count":10,"searchable_terms":["speech","audio","Whisper","transcription","VLA entry point","multilingual support","noise robustness","diarization","confidence scores","real-time processing"]},"sidebar":"docs","previous":{"title":"Module 4: Vision-Language-Action Pipeline","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/docs/module4/intro"},"next":{"title":"LLM Cognitive Planning","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/docs/module4/ch2-llm-planning"}}');var t=i(4848),r=i(8453);const o={title:"Speech Recognition with Whisper",module:4,chapter:1,id:"ch1-whisper",sidebar_position:1,learning_objectives:["Explain Whisper's role in the VLA pipeline as the voice-to-text entry point","Understand multilingual speech recognition capabilities, noise robustness, and limitations","Recognize why transcription alone is insufficient for robot understanding and control"],prerequisites:["Module 1: ROS 2 Fundamentals completed"],related_chapters:["chapter2-llm-planning","chapter4-complete-vla"],keywords:["speech recognition","Whisper","audio transcription","VLA pipeline","multilingual support","noise robustness"],difficulty:"Beginner",estimated_reading_time:"15 minutes",estimated_word_count:5e3,created_at:"2025-12-08",chunk_count:10,searchable_terms:["speech","audio","Whisper","transcription","VLA entry point","multilingual support","noise robustness","diarization","confidence scores","real-time processing"]},l="Chapter 1: Speech Recognition with Whisper",c={},d=[{value:"What is Speech Recognition?",id:"what-is-speech-recognition",level:2},{value:"Why It&#39;s Hard",id:"why-its-hard",level:3},{value:"Introducing Whisper",id:"introducing-whisper",level:2},{value:"Why Whisper for Robots?",id:"why-whisper-for-robots",level:3},{value:"How Whisper Works (Conceptually)",id:"how-whisper-works-conceptually",level:2},{value:"Step 1: Convert Audio to Features",id:"step-1-convert-audio-to-features",level:3},{value:"Step 2: Process through Neural Network",id:"step-2-process-through-neural-network",level:3},{value:"Step 3: Output Most Likely Text",id:"step-3-output-most-likely-text",level:3},{value:"Whisper in Practice: Real-World Examples",id:"whisper-in-practice-real-world-examples",level:2},{value:"Example 1: Kitchen Noise",id:"example-1-kitchen-noise",level:3},{value:"Example 2: Accent Variation",id:"example-2-accent-variation",level:3},{value:"Example 3: Multilingual",id:"example-3-multilingual",level:3},{value:"Whisper Capabilities",id:"whisper-capabilities",level:2},{value:"What Whisper Does Well",id:"what-whisper-does-well",level:3},{value:"What Whisper Struggles With",id:"what-whisper-struggles-with",level:3},{value:"Whisper Output: Not Just Text",id:"whisper-output-not-just-text",level:2},{value:"Whisper Limitations (Critical for Robots)",id:"whisper-limitations-critical-for-robots",level:2},{value:"Limitation 1: Not Understanding (Just Transcription)",id:"limitation-1-not-understanding-just-transcription",level:3},{value:"Limitation 2: No Real-Time Streaming (for most models)",id:"limitation-2-no-real-time-streaming-for-most-models",level:3},{value:"Limitation 3: No Action from Speech",id:"limitation-3-no-action-from-speech",level:3},{value:"Why Transcription Isn&#39;t Understanding",id:"why-transcription-isnt-understanding",level:2},{value:"Real-World Scenario: Robot at Dinner Table",id:"real-world-scenario-robot-at-dinner-table",level:2},{value:"User Command",id:"user-command",level:3},{value:"Step 1: Audio Capture",id:"step-1-audio-capture",level:3},{value:"Step 2: Whisper Transcription",id:"step-2-whisper-transcription",level:3},{value:"Step 3: What Whisper Knows",id:"step-3-what-whisper-knows",level:3},{value:"Step 4: What Whisper Doesn&#39;t Know",id:"step-4-what-whisper-doesnt-know",level:3},{value:"Step 5: Next Steps",id:"step-5-next-steps",level:3},{value:"Whisper in the VLA Pipeline",id:"whisper-in-the-vla-pipeline",level:2},{value:"Comparing Speech Recognition Systems",id:"comparing-speech-recognition-systems",level:2},{value:"Integrating Whisper with ROS 2",id:"integrating-whisper-with-ros-2",level:2},{value:"Voice Input Node Architecture",id:"voice-input-node-architecture",level:3},{value:"Message Flow Example",id:"message-flow-example",level:3},{value:"Action Server Integration Preview",id:"action-server-integration-preview",level:3},{value:"Accessibility and Real-World Impact",id:"accessibility-and-real-world-impact",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Edge Cases and Limitations to Remember",id:"edge-cases-and-limitations-to-remember",level:2},{value:"Next: Chapter 2",id:"next-chapter-2",level:2}];function a(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"chapter-1-speech-recognition-with-whisper",children:"Chapter 1: Speech Recognition with Whisper"})}),"\n",(0,t.jsx)(n.h2,{id:"what-is-speech-recognition",children:"What is Speech Recognition?"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Speech recognition"})," converts spoken audio into written text. The process:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'Audio Input (microphone)\n    \u2193\nAcoustic Processing (extract sound features)\n    \u2193\nLanguage Model (predict words from features)\n    \u2193\nText Output ("Pick up the blue object")\n'})}),"\n",(0,t.jsx)(n.h3,{id:"why-its-hard",children:"Why It's Hard"}),"\n",(0,t.jsxs)(n.p,{children:["Humans easily understand speech because we have ",(0,t.jsx)(n.strong,{children:"context"}),". Machines struggle with:"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Accents"}),": Different people pronounce words differently"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Noise"}),": Background sounds (traffic, crowds, machinery)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Speed"}),": Fast talkers, mumbling, pauses"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Ambiguity"}),': "Read" could be present (read the book) or past (I read it)']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Domain"}),": Robotics vocabulary (gripper, trajectory, endpoint)"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"introducing-whisper",children:"Introducing Whisper"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Whisper"})," is OpenAI's speech recognition model trained on 680,000 hours of multilingual audio. Key features:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"99+ languages"}),": Works globally"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Robust to noise"}),": Trained on noisy real-world audio"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"No fine-tuning needed"}),": Works out-of-the-box"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Accuracy"}),": 97% on English speech"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"why-whisper-for-robots",children:"Why Whisper for Robots?"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"Robustness: Works in noisy environments (warehouses, kitchens)\nMultilingual: Robots can understand users in their native language\nNo training data: No need to collect thousands of hours of robot-specific audio\nReliable: Well-tested on diverse real-world data\n"})}),"\n",(0,t.jsx)(n.h2,{id:"how-whisper-works-conceptually",children:"How Whisper Works (Conceptually)"}),"\n",(0,t.jsx)(n.p,{children:"Don't worry about the math\u2014here's the intuition:"}),"\n",(0,t.jsx)(n.h3,{id:"step-1-convert-audio-to-features",children:"Step 1: Convert Audio to Features"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"Raw audio (48,000 samples/second)\n    \u2193\nSpectrogram (visual representation of sound)\n    \u2193\nLearned features (important patterns)\n"})}),"\n",(0,t.jsxs)(n.p,{children:["The spectrogram shows ",(0,t.jsx)(n.strong,{children:"what frequencies are present at what times"}),". For example:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Low frequencies: Vowels, bass"}),"\n",(0,t.jsx)(n.li,{children:"High frequencies: Consonants, sibilants"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"step-2-process-through-neural-network",children:"Step 2: Process through Neural Network"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"Features\n    \u2193\nDeep learning network (trained on 680k hours)\n    \u2193\nPredictions for each word\n"})}),"\n",(0,t.jsx)(n.p,{children:"The network learns patterns like:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:'"Th" sound often followed by vowel'}),"\n",(0,t.jsx)(n.li,{children:'"ing" sound ends many verbs'}),"\n",(0,t.jsx)(n.li,{children:'"robot" appears in robotics contexts'}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"step-3-output-most-likely-text",children:"Step 3: Output Most Likely Text"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'Network computes probabilities:\n"Pick" (99%)\n"Pic" (0.5%)\n"Pik" (0.3%)\n    \u2193\nChoose "Pick" (highest probability)\n'})}),"\n",(0,t.jsx)(n.h2,{id:"whisper-in-practice-real-world-examples",children:"Whisper in Practice: Real-World Examples"}),"\n",(0,t.jsx)(n.h3,{id:"example-1-kitchen-noise",children:"Example 1: Kitchen Noise"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'Audio: "Pick UP the BLUE object" (normal speech in noisy kitchen)\n\nWhisper processes:\n  - Recognizes "Pick UP" despite dishwasher noise\n  - Understands "the BLUE" from context\n  - Outputs: "Pick up the blue object"\n\nHuman interpretation: "Pick up that blue thing"\nWhisper interpretation: Exact same!\n'})}),"\n",(0,t.jsx)(n.h3,{id:"example-2-accent-variation",children:"Example 2: Accent Variation"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'User 1 (American accent): "pick uh the blue object"\nUser 2 (British accent): "pick up the bloo object"\nUser 3 (Indian accent): "pick up ze blue objet"\n\nWhisper trained on diverse speakers:\n  - Recognizes all three as: "Pick up the blue object"\n  - Same output regardless of accent!\n'})}),"\n",(0,t.jsx)(n.h3,{id:"example-3-multilingual",children:"Example 3: Multilingual"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'English: "Pick up the blue object" \u2192 English text\nSpanish: "Recoge el objeto azul" \u2192 Spanish text\nMandarin: "\u62ff\u8d77\u84dd\u8272\u7269\u4f53" \u2192 Mandarin text\n\nSame Whisper model handles all languages!\n'})}),"\n",(0,t.jsx)(n.h2,{id:"whisper-capabilities",children:"Whisper Capabilities"}),"\n",(0,t.jsx)(n.h3,{id:"what-whisper-does-well",children:"What Whisper Does Well"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Capability"}),(0,t.jsx)(n.th,{children:"Example"}),(0,t.jsx)(n.th,{children:"Success Rate"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Clean speech"})}),(0,t.jsx)(n.td,{children:'"Pick up the blue object" (quiet room)'}),(0,t.jsx)(n.td,{children:"99%+"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Noisy speech"})}),(0,t.jsx)(n.td,{children:'"Pick up the blue object" (crowded room)'}),(0,t.jsx)(n.td,{children:"95%+"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Accents"})}),(0,t.jsx)(n.td,{children:'"Pick up..." in any accent'}),(0,t.jsx)(n.td,{children:"97%+"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Technical terms"})}),(0,t.jsx)(n.td,{children:'"Move to waypoint 5"'}),(0,t.jsx)(n.td,{children:"85-90%"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Multiple speakers"})}),(0,t.jsx)(n.td,{children:'"Robot, pick up... [robot responds]"'}),(0,t.jsx)(n.td,{children:"80-90%"})]})]})]}),"\n",(0,t.jsx)(n.h3,{id:"what-whisper-struggles-with",children:"What Whisper Struggles With"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Challenge"}),(0,t.jsx)(n.th,{children:"Example"}),(0,t.jsx)(n.th,{children:"Success Rate"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Very noisy"})}),(0,t.jsx)(n.td,{children:'"Pick up..." (jackhammer nearby)'}),(0,t.jsx)(n.td,{children:"50-70%"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Domain-specific jargon"})}),(0,t.jsx)(n.td,{children:'"Set PID gains to 0.5"'}),(0,t.jsx)(n.td,{children:"60-80%"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Accented heavily"})}),(0,t.jsx)(n.td,{children:"Heavily accented technical speech"}),(0,t.jsx)(n.td,{children:"70-85%"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Background speech"})}),(0,t.jsx)(n.td,{children:"Multiple people talking"}),(0,t.jsx)(n.td,{children:"40-70%"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Whispers"})}),(0,t.jsx)(n.td,{children:"Very quiet speech"}),(0,t.jsx)(n.td,{children:"50-80%"})]})]})]}),"\n",(0,t.jsx)(n.h2,{id:"whisper-output-not-just-text",children:"Whisper Output: Not Just Text"}),"\n",(0,t.jsx)(n.p,{children:"Whisper outputs more than just transcribed text:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'{\n  "text": "Pick up the blue object",\n  "language": "English",\n  "duration": 2.3,  // seconds\n  "confidence": 0.95,  // 95% confident\n  "words": [\n    {"word": "Pick", "start": 0.0, "end": 0.4},\n    {"word": "up", "start": 0.4, "end": 0.7},\n    ...\n  ]\n}\n'})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Key insight"}),": Confidence scores tell you when Whisper is uncertain!"]}),"\n",(0,t.jsx)(n.h2,{id:"whisper-limitations-critical-for-robots",children:"Whisper Limitations (Critical for Robots)"}),"\n",(0,t.jsx)(n.h3,{id:"limitation-1-not-understanding-just-transcription",children:"Limitation 1: Not Understanding (Just Transcription)"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'Whisper output: "Pick up the blue object"\n\nBut what if user said:\n  "Pick up the BLUE object" (emphasis on blue, not color)\n  "Pick up the blue OBJECT" (emphasis on picking, not identity)\n\nWhisper doesn\'t understand intent\u2014just transcribes!\nSolution: Chapter 2 (LLM) handles understanding\n'})}),"\n",(0,t.jsx)(n.h3,{id:"limitation-2-no-real-time-streaming-for-most-models",children:"Limitation 2: No Real-Time Streaming (for most models)"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'Whisper processes entire audio at once:\n  1. User speaks: "Pick up the blue object" (2 seconds)\n  2. After user finishes, Whisper processes\n  3. Output: "Pick up the blue object"\n\nCan\'t start processing until full audio is available\nSolution: Use specialized streaming models or multiple models\n'})}),"\n",(0,t.jsx)(n.h3,{id:"limitation-3-no-action-from-speech",children:"Limitation 3: No Action from Speech"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'Whisper output: "Pick up the blue object"\n\nBut:\n  - What blue object? (depends on context)\n  - From where? (depends on location)\n  - Put it where? (depends on goal)\n  - Gently or forcefully? (depends on material)\n\nWhisper doesn\'t know any of this!\nSolution: Chapter 2 (LLM) and Chapter 3 (ROS 2 Actions)\n'})}),"\n",(0,t.jsx)(n.h2,{id:"why-transcription-isnt-understanding",children:"Why Transcription Isn't Understanding"}),"\n",(0,t.jsx)(n.p,{children:"Here's the key insight for robotics:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'User says: "Move it"\n\nWhisper transcribes: "Move it"\n     \u2193\nBut "it" is ambiguous! Move what?\n     \u2193\nWhisper can\'t disambiguate\n     \u2193\nRobot needs more: "Move... the blue object? The red cup? What?"\n     \u2193\nSolution: Language understanding (Chapter 2)\n'})}),"\n",(0,t.jsx)(n.h2,{id:"real-world-scenario-robot-at-dinner-table",children:"Real-World Scenario: Robot at Dinner Table"}),"\n",(0,t.jsx)(n.p,{children:"Let's trace how Whisper fits into a complete system:"}),"\n",(0,t.jsx)(n.h3,{id:"user-command",children:"User Command"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'"Robot, pour me a glass of water"\n'})}),"\n",(0,t.jsx)(n.h3,{id:"step-1-audio-capture",children:"Step 1: Audio Capture"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"Microphone captures audio\nDuration: 2 seconds\nSample rate: 16 kHz (standard)\n"})}),"\n",(0,t.jsx)(n.h3,{id:"step-2-whisper-transcription",children:"Step 2: Whisper Transcription"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'Input: Raw audio\nOutput: "Robot, pour me a glass of water"\nConfidence: 98%\n'})}),"\n",(0,t.jsx)(n.h3,{id:"step-3-what-whisper-knows",children:"Step 3: What Whisper Knows"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"\u2705 User wants something poured"}),"\n",(0,t.jsx)(n.li,{children:"\u2705 Target is a glass"}),"\n",(0,t.jsx)(n.li,{children:"\u2705 Content is water"}),"\n",(0,t.jsx)(n.li,{children:'\u2705 Recipient is user ("me")'}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"step-4-what-whisper-doesnt-know",children:"Step 4: What Whisper Doesn't Know"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"\u274c Which glass? (multiple on table)"}),"\n",(0,t.jsx)(n.li,{children:"\u274c How full? (to brim? halfway?)"}),"\n",(0,t.jsx)(n.li,{children:"\u274c Water location? (pitcher? tap?)"}),"\n",(0,t.jsx)(n.li,{children:"\u274c Is this possible? (robot has gripper, not pouring mechanism)"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"step-5-next-steps",children:"Step 5: Next Steps"}),"\n",(0,t.jsx)(n.p,{children:"\u2192 Chapter 2 (LLM) handles disambiguation\n\u2192 Chapter 3 (ROS 2 Actions) handles execution"}),"\n",(0,t.jsx)(n.h2,{id:"whisper-in-the-vla-pipeline",children:"Whisper in the VLA Pipeline"}),"\n",(0,t.jsx)(n.p,{children:"Here's where Whisper fits in the complete system:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'User speaks\n    \u2193\nWhisper (Chapter 1)\n"Pick up the blue object"\n    \u2193\nLLM (Chapter 2)\n{action: pick_up, object: blue_ball}\n    \u2193\nROS 2 Actions (Chapter 3)\nPlan trajectory \u2192 Execute \u2192 Move robot arm\n    \u2193\nFeedback (Chapters 3-4)\nLiDAR says object grasped \u2192 Success!\n'})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Whisper's role"}),": Reliable voice \u2192 text conversion"]}),"\n",(0,t.jsx)(n.h2,{id:"comparing-speech-recognition-systems",children:"Comparing Speech Recognition Systems"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"System"}),(0,t.jsx)(n.th,{children:"Accuracy"}),(0,t.jsx)(n.th,{children:"Noise Robust"}),(0,t.jsx)(n.th,{children:"Multilingual"}),(0,t.jsx)(n.th,{children:"Real-time"}),(0,t.jsx)(n.th,{children:"Cost"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Whisper"})}),(0,t.jsx)(n.td,{children:"97%"}),(0,t.jsx)(n.td,{children:"Very"}),(0,t.jsx)(n.td,{children:"Yes (99+)"}),(0,t.jsx)(n.td,{children:"No*"}),(0,t.jsx)(n.td,{children:"Free/$$"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Google Cloud"})}),(0,t.jsx)(n.td,{children:"95%"}),(0,t.jsx)(n.td,{children:"Good"}),(0,t.jsx)(n.td,{children:"Yes (100+)"}),(0,t.jsx)(n.td,{children:"Yes"}),(0,t.jsx)(n.td,{children:"$$"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Azure Speech"})}),(0,t.jsx)(n.td,{children:"96%"}),(0,t.jsx)(n.td,{children:"Good"}),(0,t.jsx)(n.td,{children:"Yes (80+)"}),(0,t.jsx)(n.td,{children:"Yes"}),(0,t.jsx)(n.td,{children:"$$"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Amazon Lex"})}),(0,t.jsx)(n.td,{children:"92%"}),(0,t.jsx)(n.td,{children:"Fair"}),(0,t.jsx)(n.td,{children:"Yes (10)"}),(0,t.jsx)(n.td,{children:"Yes"}),(0,t.jsx)(n.td,{children:"$$"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Local models"})}),(0,t.jsx)(n.td,{children:"85%"}),(0,t.jsx)(n.td,{children:"Fair"}),(0,t.jsx)(n.td,{children:"No (1-3)"}),(0,t.jsx)(n.td,{children:"Yes"}),(0,t.jsx)(n.td,{children:"Free"})]})]})]}),"\n",(0,t.jsx)(n.p,{children:"*Streaming versions available"}),"\n",(0,t.jsx)(n.h2,{id:"integrating-whisper-with-ros-2",children:"Integrating Whisper with ROS 2"}),"\n",(0,t.jsxs)(n.p,{children:["Now let's connect Whisper to the robot system using ",(0,t.jsx)(n.a,{href:"/module1/ch1-ros2-core",children:"ROS 2 concepts from Module 1"}),":"]}),"\n",(0,t.jsx)(n.h3,{id:"voice-input-node-architecture",children:"Voice Input Node Architecture"}),"\n",(0,t.jsx)(n.p,{children:"In a real humanoid robot system, you'd structure Whisper as a ROS 2 service or topic:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Robot Voice Input                   \u2502\n\u2502 (Microphone + Audio Thread)         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502 Audio Stream\n           \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Whisper Node                        \u2502\n\u2502 (Transcription Service)             \u2502\n\u2502                                     \u2502\n\u2502 - Receives: /audio/raw (topic)     \u2502\n\u2502 - Processes: Audio \u2192 Text          \u2502\n\u2502 - Outputs: /speech/transcribed     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502 Text Data\n           \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 LLM Planning Node (Chapter 2)       \u2502\n\u2502 Receives transcription, plans action\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Key ROS 2 Concepts"})," (Review ",(0,t.jsx)(n.a,{href:"/module1/ch1-ros2-core",children:"Module 1 fundamentals"})," for details):"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Nodes"}),": Whisper runs as a single ROS 2 node processing audio"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Topics"}),": Audio data flows on ",(0,t.jsx)(n.code,{children:"/audio/raw"}),", transcription on ",(0,t.jsx)(n.code,{children:"/speech/transcribed"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Messages"}),": Audio messages contain raw PCM bytes; speech messages contain text strings with confidence"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Services"}),": Whisper can also be a service node when you need synchronous transcription"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Python Agents"}),": You'd use ",(0,t.jsx)(n.a,{href:"/module1/ch2-agent-bridge",children:"ROS 2 Python agents from Module 1"})," to coordinate between Whisper and LLM nodes"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"message-flow-example",children:"Message Flow Example"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# Pseudocode: How a Python agent coordinates Whisper\n# (Reference: Module 1 Chapter 2 - Python Agents)\n\nclass VoiceToActionAgent:\n    def __init__(self):\n        # Subscribe to microphone topic (Module 1: Topics)\n        self.audio_subscriber = rospy.Subscriber('/audio/raw', AudioMessage, self.on_audio)\n\n        # Create Whisper client (Module 1: Services)\n        self.whisper_client = rospy.ServiceProxy('whisper_transcribe', WhisperService)\n\n        # Publish transcription (Module 1: Topics)\n        self.speech_publisher = rospy.Publisher('/speech/transcribed', String)\n\n    def on_audio(self, audio_msg):\n        # Call Whisper service with audio data\n        response = self.whisper_client(audio=audio_msg.data)\n\n        # Publish result\n        self.speech_publisher.publish(response.transcription)\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Note"}),": This is conceptual pseudocode showing how ROS 2 components connect. For actual implementation details, see ",(0,t.jsx)(n.a,{href:"/module1/ch2-agent-bridge",children:"Module 1 Python Agent Bridge"}),"."]}),"\n",(0,t.jsx)(n.h3,{id:"action-server-integration-preview",children:"Action Server Integration Preview"}),"\n",(0,t.jsxs)(n.p,{children:["Once Whisper transcribes text, the next layer is the ",(0,t.jsx)(n.a,{href:"/module4/ch3-ros2-actions",children:"ROS 2 Action Server"})," covered in Chapter 3. The complete flow:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"Audio Input\n    \u2193 (Whisper: Chapter 1)\nText Output\n    \u2193 (LLM: Chapter 2)\nStructured Plan\n    \u2193 (Action Server: Chapter 3)\nRobot Motion\n"})}),"\n",(0,t.jsxs)(n.p,{children:["For more detail on ROS 2 Actions, see ",(0,t.jsx)(n.a,{href:"/module4/ch3-ros2-actions",children:"Chapter 3: ROS 2 Actions"})," and ",(0,t.jsx)(n.a,{href:"/module1/ch1-ros2-core",children:"Module 1 Core Concepts"}),"."]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"accessibility-and-real-world-impact",children:"Accessibility and Real-World Impact"}),"\n",(0,t.jsx)(n.p,{children:"Beyond robotics, Whisper enables accessibility:"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"For Users with Speech Disabilities"}),": Voice commands make systems accessible to people who can't use keyboards."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"For Meeting Transcription"}),": Real-time transcription of meetings, lectures, podcasts in multiple languages."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"For Education"}),": Students can speak notes instead of typing, helping those with motor disabilities."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"For Multilingual Teams"}),": Teams speaking different languages can have conversations with real-time translation support."]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,t.jsxs)(n.p,{children:["\u2713 ",(0,t.jsx)(n.strong,{children:"Speech recognition"})," converts audio to text using machine learning\n\u2713 ",(0,t.jsx)(n.strong,{children:"Whisper"})," is robust to noise, accents, and supports 99+ languages\n\u2713 ",(0,t.jsx)(n.strong,{children:"97% accuracy"})," on English makes it reliable for robot voice commands\n\u2713 ",(0,t.jsx)(n.strong,{children:"Transcription \u2260 understanding"})," - Whisper transcribes speech, but doesn't comprehend meaning\n\u2713 ",(0,t.jsx)(n.strong,{children:"Limitations"}),": No context understanding, no real-time streaming in base version, no action capability\n\u2713 ",(0,t.jsx)(n.strong,{children:"ROS 2 integration"}),": Whisper runs as a node/service in the robot system (see ",(0,t.jsx)(n.a,{href:"/module1/ch1-ros2-core",children:"Module 1 ROS 2 concepts"}),")\n\u2713 ",(0,t.jsx)(n.strong,{children:"Next step (Chapter 2)"}),": Language understanding via LLM to extract intent and entities\n\u2713 ",(0,t.jsx)(n.strong,{children:"Complete system"}),": Chapter 4 shows how Whisper + LLM + ROS 2 Actions work together"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"edge-cases-and-limitations-to-remember",children:"Edge Cases and Limitations to Remember"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Homophone Ambiguity"}),': Words that sound the same ("there" vs "their" vs "they\'re") - Whisper chooses based on training data likelihood']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Domain-Specific Vocabulary"}),': Robotics terms like "endpoint" or "trajectory" may be transcribed as common words ("end point", "tray jectory")']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Background Voices"}),": Multiple speakers create confusion; Whisper may pick up conversations near the robot"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Streaming Constraints"}),": Standard Whisper waits for complete audio; real-time applications need specialized models"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Confidence Scores"}),": Always check Whisper's confidence metric; low scores (< 60%) may indicate transcription errors"]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"next-chapter-2",children:"Next: Chapter 2"}),"\n",(0,t.jsxs)(n.p,{children:["Whisper gives you text. But what does the text mean? In ",(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.a,{href:"/module4/ch2-llm-planning",children:"Chapter 2: LLM Planning"})}),", you'll learn how robots ",(0,t.jsx)(n.strong,{children:"understand intent"})," from that text using large language models."]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Learning Outcome"}),": You now understand how Whisper converts speech to text reliably, why it's useful for humanoid robots, its integration with ROS 2, and why transcription alone isn't enough for robot understanding and action planning."]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(a,{...e})}):a(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>l});var s=i(6540);const t={},r=s.createContext(t);function o(e){const n=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),s.createElement(r.Provider,{value:n},e.children)}}}]);